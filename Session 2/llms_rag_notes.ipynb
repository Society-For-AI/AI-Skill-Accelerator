{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c8e03dc",
   "metadata": {},
   "source": [
    "üìö Foundational Models, Prompt/Context Engineering, RAG Systems\n",
    "Author: Emmanuel Obute\n",
    "Connect: Linkedin\n",
    "Level: Beginner to Intermediate\n",
    "Duration: 1-2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1c64c",
   "metadata": {},
   "source": [
    "üß† 1. Foundational Models\n",
    "‚úÖ Definition:\n",
    "Foundational models are large-scale AI models trained on massive and diverse datasets, designed to be general-purpose and adaptable to many downstream tasks (e.g., chat, summarization, coding).\n",
    "\n",
    "Large pretrained models (e.g., GPT, LLaMA, Mistral) that can handle diverse language tasks without task-specific training.\n",
    "\n",
    "üîç Characteristics:\n",
    "Trained using self-supervised learning\n",
    "\n",
    "Can be fine-tuned or used as-is (zero-shot/few-shot)\n",
    "\n",
    "Examples: GPT-4, LLaMA, Mistral, Gemma, Claude, PaLM, Phi\n",
    "\n",
    "üìå Why Important:\n",
    "Serve as the backbone for applications like chatbots, RAG systems, and agent frameworks.\n",
    "\n",
    "Reduce need for task-specific models.\n",
    "\n",
    "üìö Common Open Models:\n",
    "| Model     | Developer  | Size        | Notes                        |\n",
    "| --------- | ---------- | ----------- | ---------------------------- |\n",
    "| GPT-4     | OpenAI     | \\~Trillion? | Closed, SOTA                 |\n",
    "| LLaMA 2/3 | Meta       | 7B‚Äì65B      | Open-weight, strong base     |\n",
    "| Mistral   | Mistral AI | 7B          | Open, fast, high quality     |\n",
    "| Phi-3     | Microsoft  | 3B          | Lightweight, efficient       |\n",
    "| Gemma     | Google     | 2B‚Äì7B       | Open-weight, tuned for speed |\n",
    "\n",
    "\n",
    "\n",
    "When to Use What?\n",
    "| Scenario                                             | Recommended Model    |\n",
    "| ---------------------------------------------------- | -------------------- |\n",
    "| Cutting-edge open model, good quality & open weights | **Mistral 7B**       |\n",
    "| Research & fine-tuning base models                   | **LLaMA 13B or 33B** |\n",
    "| Local fast chat with smaller footprint               | **Gemma or Phi-3**   |\n",
    "| Lightweight and speedy local inference               | **Phi-3**            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d8ecc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Using GPT-3.5 Turbo with OpenAI\n",
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your-api-key\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the theory of relativity?\"}\n",
    "    ],\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649a0d2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Using Mistral via Ollama (locally)\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "print(llm(\"Explain Newton's First Law of Motion.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803337d8",
   "metadata": {},
   "source": [
    "üí¨ 2. Prompt / Context Engineering\n",
    "‚úÖ Definition:\n",
    "Prompt or context engineering is the process of designing inputs to guide LLMs towards desired outputs without fine-tuning the model itself.\n",
    "\n",
    "Crafting input prompts that guide LLMs toward accurate or desirable outputs.\n",
    "\n",
    "üß∞ Techniques:\n",
    "Technique\tDescription\n",
    "Zero-shot\tAsk directly without examples\n",
    "Few-shot\tInclude 1‚Äì5 examples to guide behavior\n",
    "Chain-of-thought\tInclude reasoning steps in prompt\n",
    "Role prompting\tGive model an identity or goal (e.g., ‚ÄúYou are a helpful assistant‚Ä¶‚Äù)\n",
    "Formatting\tUse consistent structure (lists, markdown, etc.)\n",
    "\n",
    "üì¶ Prompt Components:\n",
    "Instructions: What to do\n",
    "\n",
    "Context: Background info\n",
    "\n",
    "Examples: Show expected behavior\n",
    "\n",
    "Input: The actual query/data\n",
    "\n",
    "‚ö†Ô∏è Challenges:\n",
    "LLMs are sensitive to wording\n",
    "\n",
    "Token limits constrain long inputs\n",
    "\n",
    "Output is non-deterministic at higher temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f9299",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Zero-shot Prompt\n",
    "\n",
    "prompt = \"Summarize this: Artificial Intelligence is transforming industries...\"\n",
    "\n",
    "\n",
    "# Example 2: Few-shot Prompt\n",
    "\n",
    "prompt = \"\"\"\n",
    "Q: What is the capital of France?\n",
    "A: Paris\n",
    "Q: What is the capital of Japan?\n",
    "A: Tokyo\n",
    "Q: What is the capital of Canada?\n",
    "A: Ontario\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037fe8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example with OpenAI\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"You are an expert summarizer.\n",
    "        Summarize the following text in 2 bullet points:\n",
    "        'The Great Wall of China is over 13,000 miles long and was built over centuries to protect Chinese territories from invasions.'\"\"\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1350c679",
   "metadata": {},
   "source": [
    "üîç 3. RAG Systems (Retrieval-Augmented Generation)\n",
    "‚úÖ Definition:\n",
    "RAG combines LLMs with external knowledge (retrieved in real time) to improve the factual accuracy and relevance of answers.\n",
    "\n",
    "üîÅ Process Flow:\n",
    "User Query\n",
    "\n",
    "Retriever fetches relevant documents from a vector database (e.g. Chroma, FAISS)\n",
    "\n",
    "Generator (LLM) uses retrieved context to answer\n",
    "\n",
    "üîß Key Components:\n",
    "Component\tRole\n",
    "Vector Store\tStores embeddings for retrieval (Chroma, FAISS)\n",
    "Embeddings\tTurns text into vectors (e.g., all-MiniLM-L6-v2)\n",
    "Retriever\tFinds top-K relevant chunks\n",
    "LLM\tGenerates answers using the context\n",
    "\n",
    "üß† Why Use RAG?\n",
    "Mitigates hallucination\n",
    "\n",
    "Adds real-time or proprietary knowledge\n",
    "\n",
    "Keeps the model factual without fine-tuning\n",
    "\n",
    "üìå Tools:\n",
    "LangChain or LlamaIndex for orchestration\n",
    "\n",
    "OpenAI, Ollama, Hugging Face models\n",
    "\n",
    "FastAPI to serve RAG as an API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4877e9",
   "metadata": {},
   "source": [
    "üìù FastAPI ‚Äì Notes\n",
    "üìå Overview:\n",
    "FastAPI is a modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.\n",
    "\n",
    "Created by Sebasti√°n Ram√≠rez\n",
    "\n",
    "Built on top of Starlette (for web handling) and Pydantic (for data validation)\n",
    "\n",
    "Designed for building RESTful APIs quickly and efficiently\n",
    "\n",
    "üöÄ Key Features:\n",
    "Fast: One of the fastest Python frameworks thanks to ASGI and async support.\n",
    "\n",
    "Type Hints: Uses Python type hints for request data validation, editor autocompletion, and documentation.\n",
    "\n",
    "Automatic Docs: Generates OpenAPI and Swagger UI automatically at:\n",
    "\n",
    "/docs (Swagger UI)\n",
    "\n",
    "/redoc (ReDoc)\n",
    "\n",
    "Async Support: Supports async/await natively for non-blocking I/O operations.\n",
    "\n",
    "Data Validation: Powered by Pydantic for strong validation and serialization.\n",
    "\n",
    "Dependency Injection: Clean way to manage dependencies in routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64123647",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Hello, FastAPI!\"}\n",
    "\n",
    "@app.get(\"/items/{item_id}\")\n",
    "def read_item(item_id: int, q: str = None):\n",
    "    return {\"item_id\": item_id, \"q\": q}\n",
    "\n",
    "\n",
    "#run: uvicorn main:app --reload"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
