{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Natural Language Processing (NLP) Fundamentals 2\n",
    "\n",
    "**Author:** Chigozilai Kejeh  \n",
    "**Connect:** [Linkedin](https://www.linkedin.com/in/chigozilai-kejeh-058014143/)  \n",
    "**Level:** Beginner to Intermediate  \n",
    "**Duration:** 1-2 hours  \n",
    "\n",
    "Welcome to Day 2 of your journey through Natural Language Processing and Large Language Models! This interactive guide will take you from basic text vectorization to understanding cutting-edge transformer architectures.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Text Vectorization & Embeddings](#vectorization)\n",
    "2. [Word2Vec & GloVe](#word2vec-glove)\n",
    "3. [Neural Networks for NLP](#neural-networks)\n",
    "4. [Sequence Models: RNNs & LSTMs](#sequence-models)\n",
    "5. [Encoder-Decoder Architecture](#encoder-decoder)\n",
    "6. [Transformers Revolution](#transformers)\n",
    "7. [BERT: Bidirectional Understanding](#bert)\n",
    "8. [GPT: Generative Pre-training](#gpt)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Text Vectorization & Embeddings\n",
    "\n",
    "### Why Vectorize Text?\n",
    "\n",
    "Computers understand numbers, not words. Text vectorization converts human language into numerical representations that machines can process.\n",
    "\n",
    "### Basic Vectorization Methods\n",
    "\n",
    "#### One-Hot Encoding\n",
    "The simplest approach - each word gets a unique position in a vector.\n",
    "\n",
    "```python\n",
    "# Example: One-hot encoding\n",
    "vocabulary = [\"cat\", \"dog\", \"bird\", \"fish\"]\n",
    "sentence = \"cat\"\n",
    "\n",
    "# One-hot vector for \"cat\"\n",
    "cat_vector = [1, 0, 0, 0]  # Position 0 for \"cat\"\n",
    "dog_vector = [0, 1, 0, 0]  # Position 1 for \"dog\"\n",
    "```\n",
    "\n",
    "**Problems with One-Hot:**\n",
    "- Sparse vectors (mostly zeros)\n",
    "- No semantic relationships\n",
    "- Vocabulary size = vector dimension\n",
    "\n",
    "#### Bag of Words (BoW)\n",
    "Count word frequencies in documents.\n",
    "\n",
    "```python\n",
    "# Example: Bag of Words\n",
    "document1 = \"the cat sat on the mat\"\n",
    "document2 = \"the dog ran in the park\"\n",
    "\n",
    "# Vocabulary: [the, cat, sat, on, mat, dog, ran, in, park]\n",
    "doc1_bow = [2, 1, 1, 1, 1, 0, 0, 0, 0]  # \"the\" appears 2 times\n",
    "doc2_bow = [2, 0, 0, 0, 0, 1, 1, 1, 1]  # \"the\" appears 2 times\n",
    "```\n",
    "\n",
    "![bow](img/bow.png)\n",
    "\n",
    "#### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "Weighs words by importance across documents.\n",
    "\n",
    "![tf-idf](img/tf-idf.png)\n",
    "![tf-idf2](img/tf-idf2.png)\n",
    "\n",
    "### Dense Embeddings\n",
    "\n",
    "Dense embeddings represent words as continuous vectors where similar words have similar representations.\n",
    "\n",
    "Instead of having a separate dimension for each word (BoW, TF-IDF), these methods represent words in a continuous vector space where each word is described by a fixed number of dimensions (typically 100-300). These vectors are dense because they contain meaningful information in every dimension, capturing semantic relationships between words.\n",
    "\n",
    "```\n",
    "Traditional: \"king\" = [0,0,1,0,0,0...] (sparse, 10,000+ dims)\n",
    "Embedding:   \"king\" = [0.2, -0.1, 0.8, 0.3, -0.5] (dense, ~300 dims)\n",
    "```\n",
    "\n",
    "**Key Properties:**\n",
    "- Lower dimensional (typically 50-1000 dimensions). Sparse vectors can be very large depending on the vocabulary\n",
    "- Capture semantic relationships.\n",
    "- Similar words cluster together in vector space. Dense embeddings, encode relationships, allowing models to understand that \"king\" is to \"queen\" as \"man\" is to \"woman,\" enabling analogies and more nuanced language understanding.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Word2Vec & GloVe \n",
    "\n",
    "### Word2Vec: Learning from Context\n",
    "\n",
    "Word2Vec learns word representations by predicting context words or target words. Word2Vec learns embeddings by training a neural network to predict missing words (CBOW) or their surrounding context (Skip-Gram), capturing how often words appear together in a low-dimensional vector space. \n",
    "\n",
    "#### CBOW (Continuous Bag of Words)\n",
    "Given context words, predict the target word.\n",
    "\n",
    "```\n",
    "Context: [\"The\", \"quick\", \"fox\", \"jumps\"] ‚Üí Target: \"brown\"\n",
    "```\n",
    "![CBOW](img/CBOW.png)\n",
    "\n",
    "#### Skip-gram Model\n",
    "Given a word, predict surrounding context words.\n",
    "\n",
    "```\n",
    "Sentence: \"The quick brown fox jumps\"\n",
    "Target: \"brown\" ‚Üí Context: [\"The\", \"quick\", \"fox\", \"jumps\"]\n",
    "```\n",
    "![skipg](img/skip-gram.png)\n",
    "\n",
    "#### Amazing Word2Vec Properties\n",
    "\n",
    "```python\n",
    "# Vector arithmetic examples\n",
    "king - man + woman ‚âà queen\n",
    "paris - france + italy ‚âà rome\n",
    "walking - walk + swim ‚âà swimming\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### GloVe: Global Vectors\n",
    "\n",
    "GloVe combines global matrix factorization with local context window methods. GloVe takes a broader approach by factorizing the global word co-occurrence matrix to derive low-dimensional embeddings. By doing so, it uncovers patterns and relationships that might be invisible from a narrow, sentence-by-sentence view.\n",
    "\n",
    "![glove](img/glove.png)\n",
    "\n",
    "#### Key Insight\n",
    "Word meanings are captured by ratios of co-occurrence probabilities.\n",
    "\n",
    "```python\n",
    "# GloVe intuition: word ratios reveal relationships\n",
    "# P(ice|solid) / P(ice|gas) = high (ice is more related to solid)\n",
    "# P(steam|solid) / P(steam|gas) = low (steam is more related to gas)\n",
    "```\n",
    "\n",
    "#### GloVe Training Process\n",
    "\n",
    "1. **Build Co-occurrence Matrix**: Count how often words appear together\n",
    "2. **Factorize Matrix**: Learn embeddings that reconstruct co-occurrence ratios\n",
    "3. **Objective Function**: Minimize difference between dot product and log co-occurrence\n",
    "\n",
    "```python\n",
    "# GloVe objective (simplified)\n",
    "def glove_objective(word_i, word_j, embeddings):\n",
    "    dot_product = embeddings[word_i] @ embeddings[word_j]\n",
    "    log_cooccurrence = math.log(cooccurrence_matrix[word_i][word_j])\n",
    "    return (dot_product - log_cooccurrence) ** 2\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Neural Networks for NLP  \n",
    "\n",
    "### Why Neural Networks for Language?\n",
    "\n",
    "Traditional methods treat words as discrete symbols. Neural networks learn continuous representations and complex patterns.\n",
    "\n",
    "### Basic Neural Language Model\n",
    "\n",
    "![nn1](img/nn1.png)\n",
    "![nn2](img/nn2.png)\n",
    "![nn3](img/nn3.png)\n",
    "![nn4](img/nn4.png)\n",
    "\n",
    "### Feedforward Networks Limitations\n",
    "\n",
    "```\n",
    "Input: \"The cat sat on the exterior side of the\"\n",
    "Problem: Fixed window size, no memory of distant words\n",
    "```\n",
    "\n",
    "**Challenges:**\n",
    "- Fixed context window\n",
    "- No sequential memory\n",
    "- Position information lost\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Sequence Models: RNNs & LSTMs  \n",
    "\n",
    "### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs process sequences by maintaining hidden state across time steps.\n",
    "\n",
    "Unlike feed-forward neural networks‚Äîwhich process inputs in a single pass and treat each input as an isolated instance‚ÄîRNNs incorporate a looping mechanism that builds a temporal memory through a hidden state. RNNs process input sequentially‚Äîone word at a time‚Äîwhile maintaining this hidden state, a vector that accumulates information from previous time steps. \n",
    "\n",
    "```\n",
    "Architecture Flow:\n",
    "x‚ÇÅ ‚Üí [RNN] ‚Üí h‚ÇÅ ‚Üí [RNN] ‚Üí h‚ÇÇ ‚Üí [RNN] ‚Üí h‚ÇÉ\n",
    "     ‚Üì        ‚Üì        ‚Üì        ‚Üì        ‚Üì\n",
    "     y‚ÇÅ       y‚ÇÇ       y‚ÇÉ       y‚ÇÑ       y‚ÇÖ\n",
    "```\n",
    "\n",
    "\n",
    "![rnn](img/rnn.png)\n",
    "\n",
    "#### RNN Applications\n",
    "\n",
    "```python\n",
    "# 1. Language Modeling (predict next word)\n",
    "\"The cat sat on the\" ‚Üí \"mat\"\n",
    "\n",
    "# 2. Sentiment Analysis (sequence ‚Üí single output)\n",
    "\"This movie is amazing!\" ‚Üí Positive\n",
    "\n",
    "# 3. Machine Translation (sequence ‚Üí sequence)\n",
    "\"Hello world\" ‚Üí \"Hola mundo\"\n",
    "```\n",
    "\n",
    "**Vanishing Gradient Problem**\n",
    "\n",
    "When sequences become very long, RNNs can face challenges like vanishing or exploding gradients, making it difficult to learn distant relationships. \n",
    "\n",
    "**Result**: RNNs struggle with long-term dependencies.\n",
    "\n",
    "### Long Short-Term Memory (LSTM)\n",
    "\n",
    "LSTMs solve vanishing gradients with gating mechanisms.\n",
    "\n",
    "#### LSTM Components\n",
    "\n",
    "```\n",
    "Gates:\n",
    "- Forget Gate: What to remove from cell state\n",
    "- Input Gate: What new information to store\n",
    "- Output Gate: What parts of cell state to output\n",
    "```\n",
    "\n",
    "![lstm](img/lstm.png)\n",
    "\n",
    "#### LSTM Intuition\n",
    "\n",
    "```\n",
    "Cell State (C): Long-term memory highway\n",
    "Hidden State (h): Short-term working memory\n",
    "\n",
    "Example: \"The cat, which was very fluffy and loved to sleep, sat on the mat\"\n",
    "- Cell state remembers \"cat\" throughout the sentence\n",
    "- Hidden state focuses on immediate context\n",
    "```\n",
    "\n",
    "### Bidirectional RNNs\n",
    "\n",
    "Process sequences in both directions for complete context.\n",
    "\n",
    "```python\n",
    "def bidirectional_rnn(sequence):\n",
    "    # Forward pass\n",
    "    forward_outputs = []\n",
    "    h_forward = initial_state\n",
    "    for x in sequence:\n",
    "        h_forward = rnn_step(x, h_forward)\n",
    "        forward_outputs.append(h_forward)\n",
    "    \n",
    "    # Backward pass\n",
    "    backward_outputs = []\n",
    "    h_backward = initial_state\n",
    "    for x in reversed(sequence):\n",
    "        h_backward = rnn_step(x, h_backward)\n",
    "        backward_outputs.append(h_backward)\n",
    "    \n",
    "    # Combine outputs\n",
    "    combined = []\n",
    "    for i in range(len(sequence)):\n",
    "        combined.append(concat(forward_outputs[i], backward_outputs[-(i+1)]))\n",
    "    \n",
    "    return combined\n",
    "```\n",
    "### Comparision\n",
    "\n",
    "![comp](img/comp.png)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Encoder-Decoder Architecture {#encoder-decoder}\n",
    "\n",
    "### The Big Idea\n",
    "\n",
    "**Encoder**: Compress input sequence into fixed representation\n",
    "**Decoder**: Generate output sequence from representation\n",
    "\n",
    "![encoder](img/enconder.png)\n",
    "![decoder](img/decoder.png)\n",
    "\n",
    "```\n",
    "Encoder: \"Hello world\" ‚Üí [0.1, 0.3, -0.2, 0.8] (context vector)\n",
    "Decoder: [0.1, 0.3, -0.2, 0.8] ‚Üí \"Hola mundo\"\n",
    "```\n",
    "\n",
    "\n",
    "### Basic Encoder-Decoder\n",
    "\n",
    "```python\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        self.encoder = LSTM(vocab_size, hidden_size)\n",
    "        self.decoder = LSTM(vocab_size, hidden_size)\n",
    "        self.output_projection = Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def encode(self, source_sequence):\n",
    "        # Process entire source sequence\n",
    "        encoder_outputs, final_state = self.encoder(source_sequence)\n",
    "        \n",
    "        # Return final hidden state as context vector\n",
    "        return final_state\n",
    "    \n",
    "    def decode(self, context_vector, target_sequence=None):\n",
    "        # Initialize decoder with context vector\n",
    "        decoder_state = context_vector\n",
    "        outputs = []\n",
    "        \n",
    "        # Generate sequence step by step\n",
    "        current_input = START_TOKEN\n",
    "        for _ in range(max_length):\n",
    "            output, decoder_state = self.decoder(current_input, decoder_state)\n",
    "            predicted_word = self.output_projection(output)\n",
    "            outputs.append(predicted_word)\n",
    "            \n",
    "            # Use predicted word as next input (during inference)\n",
    "            current_input = predicted_word\n",
    "            \n",
    "            if predicted_word == END_TOKEN:\n",
    "                break\n",
    "        \n",
    "        return outputs\n",
    "```\n",
    "\n",
    "### Attention Mechanism\n",
    "\n",
    "The bottleneck problem: All information compressed into single vector.\n",
    "\n",
    "**Solution**: Attention allows decoder to focus on different parts of input.\n",
    "\n",
    "```python\n",
    "def attention(decoder_hidden, encoder_outputs):\n",
    "    # Compute attention scores\n",
    "    scores = []\n",
    "    for encoder_output in encoder_outputs:\n",
    "        score = dot_product(decoder_hidden, encoder_output)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    attention_weights = softmax(scores)\n",
    "    \n",
    "    # Weighted sum of encoder outputs\n",
    "    context = sum(weight * output for weight, output \n",
    "                  in zip(attention_weights, encoder_outputs))\n",
    "    \n",
    "    return context, attention_weights\n",
    "```\n",
    "\n",
    "#### Attention Visualization\n",
    "\n",
    "```\n",
    "Source: \"The cat sat on the mat\"\n",
    "Target: \"Le chat √©tait assis sur le tapis\"\n",
    "\n",
    "When generating \"chat\":\n",
    "Attention weights: [0.1, 0.8, 0.05, 0.02, 0.02, 0.01]\n",
    "                   [The, cat, sat,  on,   the,  mat]\n",
    "                        ‚Üë (highest attention on \"cat\")\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Transformers Revolution  \n",
    "\n",
    "### \"Attention Is All You Need\"\n",
    "\n",
    "Transformers replaced RNNs with pure attention mechanisms.\n",
    "\n",
    "#### Key Innovations:\n",
    "1. **Self-Attention**: Words attend to other words in same sequence\n",
    "2. **Parallel Processing**: No sequential dependency\n",
    "3. **Position Encoding**: Explicit position information\n",
    "\n",
    "### Self-Attention Mechanism\n",
    "\n",
    "```python\n",
    "def self_attention(query, key, value, mask=None):\n",
    "    # Compute attention scores\n",
    "    scores = query @ key.T / math.sqrt(key.shape[-1])  # Scaled dot-product\n",
    "    \n",
    "    # Apply mask (for padding or causality)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    attention_weights = softmax(scores)\n",
    "    \n",
    "    # Apply attention to values\n",
    "    output = attention_weights @ value\n",
    "    \n",
    "    return output, attention_weights\n",
    "```\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = Linear(d_model, d_model)\n",
    "        self.W_k = Linear(d_model, d_model)\n",
    "        self.W_v = Linear(d_model, d_model)\n",
    "        self.W_o = Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Create Q, K, V\n",
    "        Q = self.W_q(x).reshape(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        K = self.W_k(x).reshape(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        V = self.W_v(x).reshape(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        \n",
    "        # Apply attention for each head\n",
    "        attention_outputs = []\n",
    "        for head in range(self.num_heads):\n",
    "            q, k, v = Q[:, :, head, :], K[:, :, head, :], V[:, :, head, :]\n",
    "            attention_output, _ = self_attention(q, k, v)\n",
    "            attention_outputs.append(attention_output)\n",
    "        \n",
    "        # Concatenate heads and project\n",
    "        concatenated = torch.cat(attention_outputs, dim=-1)\n",
    "        output = self.W_o(concatenated)\n",
    "        \n",
    "        return output\n",
    "```\n",
    "\n",
    "### Position Encoding\n",
    "\n",
    "Since Transformers have no recurrence, position information must be added explicitly.\n",
    "\n",
    "```python\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    \n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
    "            pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n",
    "    \n",
    "    return pe\n",
    "```\n",
    "\n",
    "### Complete Transformer Block\n",
    "\n",
    "```python\n",
    "class TransformerBlock:\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = Sequential([\n",
    "            Linear(d_model, d_ff),\n",
    "            ReLU(),\n",
    "            Linear(d_ff, d_model)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7. BERT: Bidirectional Understanding {#bert}\n",
    "\n",
    "### BERT's Revolutionary Approach\n",
    "\n",
    "**Traditional**: Left-to-right or right-to-left processing\n",
    "**BERT**: Bidirectional context from both directions simultaneously\n",
    "\n",
    "### Pre-training Tasks\n",
    "\n",
    "#### 1. Masked Language Modeling (MLM)\n",
    "\n",
    "```python\n",
    "# Original sentence\n",
    "\"The cat sat on the mat\"\n",
    "\n",
    "# Masked version (15% of tokens)\n",
    "\"The [MASK] sat on the mat\"\n",
    "\n",
    "# BERT learns to predict: \"cat\"\n",
    "```\n",
    "\n",
    "#### 2. Next Sentence Prediction (NSP)\n",
    "\n",
    "```python\n",
    "# Example pairs\n",
    "Sentence A: \"The cat sat on the mat\"\n",
    "Sentence B: \"It was very comfortable\"  # IsNext: True\n",
    "\n",
    "Sentence A: \"The cat sat on the mat\"  \n",
    "Sentence B: \"Quantum physics is complex\"  # IsNext: False\n",
    "```\n",
    "\n",
    "### BERT Architecture\n",
    "\n",
    "```python\n",
    "class BERT:\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads):\n",
    "        # Token embeddings\n",
    "        self.token_embeddings = Embedding(vocab_size, hidden_size)\n",
    "        \n",
    "        # Position embeddings\n",
    "        self.position_embeddings = Embedding(max_seq_len, hidden_size)\n",
    "        \n",
    "        # Segment embeddings (for sentence pairs)\n",
    "        self.segment_embeddings = Embedding(2, hidden_size)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.layers = [TransformerBlock(hidden_size, num_heads) \n",
    "                      for _ in range(num_layers)]\n",
    "        \n",
    "        # Pre-training heads\n",
    "        self.mlm_head = Linear(hidden_size, vocab_size)\n",
    "        self.nsp_head = Linear(hidden_size, 2)\n",
    "    \n",
    "    def forward(self, input_ids, segment_ids):\n",
    "        # Create embeddings\n",
    "        token_emb = self.token_embeddings(input_ids)\n",
    "        pos_emb = self.position_embeddings(range(len(input_ids)))\n",
    "        seg_emb = self.segment_embeddings(segment_ids)\n",
    "        \n",
    "        # Sum all embeddings\n",
    "        x = token_emb + pos_emb + seg_emb\n",
    "        \n",
    "        # Pass through transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "### BERT Fine-tuning\n",
    "\n",
    "```python\n",
    "# Fine-tuning for classification\n",
    "class BERTClassifier:\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        self.bert = bert_model\n",
    "        self.classifier = Linear(bert_model.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, segment_ids):\n",
    "        # Get BERT representations\n",
    "        bert_output = self.bert(input_ids, segment_ids)\n",
    "        \n",
    "        # Use [CLS] token representation for classification\n",
    "        cls_representation = bert_output[:, 0, :]  # First token\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(cls_representation)\n",
    "        return logits\n",
    "```\n",
    "\n",
    "### BERT's Impact\n",
    "\n",
    "```\n",
    "Task Examples:\n",
    "1. Sentiment Analysis: \"This movie is [MASK]\" ‚Üí \"amazing\"\n",
    "2. Question Answering: Context + Question ‚Üí Answer span\n",
    "3. Named Entity Recognition: Identify persons, locations, organizations\n",
    "4. Text Similarity: Compare semantic similarity between sentences\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. GPT: Generative Pre-training {#gpt}\n",
    "\n",
    "### GPT's Approach: Autoregressive Generation\n",
    "\n",
    "**Key Insight**: Learn to predict next word, then use for various tasks via prompting.\n",
    "\n",
    "```python\n",
    "# Training objective\n",
    "P(w‚ÇÅ, w‚ÇÇ, ..., w‚Çô) = P(w‚ÇÅ) √ó P(w‚ÇÇ|w‚ÇÅ) √ó P(w‚ÇÉ|w‚ÇÅ,w‚ÇÇ) √ó ... √ó P(w‚Çô|w‚ÇÅ,...,w‚Çô‚Çã‚ÇÅ)\n",
    "```\n",
    "\n",
    "### GPT Architecture\n",
    "\n",
    "```python\n",
    "class GPT:\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads):\n",
    "        self.token_embeddings = Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = Embedding(max_seq_len, hidden_size)\n",
    "        \n",
    "        # Decoder-only transformer blocks\n",
    "        self.layers = [TransformerDecoderBlock(hidden_size, num_heads) \n",
    "                      for _ in range(num_layers)]\n",
    "        \n",
    "        # Language modeling head\n",
    "        self.lm_head = Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # Embeddings\n",
    "        x = self.token_embeddings(input_ids) + self.position_embeddings(range(len(input_ids)))\n",
    "        \n",
    "        # Causal attention (can only attend to previous tokens)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, causal_mask=True)\n",
    "        \n",
    "        # Predict next token probabilities\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "```\n",
    "\n",
    "### Causal (Masked) Attention\n",
    "\n",
    "```python\n",
    "def causal_attention_mask(seq_len):\n",
    "    # Create lower triangular matrix\n",
    "    mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "    return mask\n",
    "\n",
    "# Example for sequence length 4:\n",
    "# [[1, 0, 0, 0],    # Token 1 can only see itself\n",
    "#  [1, 1, 0, 0],    # Token 2 can see tokens 1-2\n",
    "#  [1, 1, 1, 0],    # Token 3 can see tokens 1-3\n",
    "#  [1, 1, 1, 1]]    # Token 4 can see all tokens\n",
    "```\n",
    "\n",
    "### Text Generation with GPT\n",
    "\n",
    "```python\n",
    "def generate_text(model, prompt, max_length=100):\n",
    "    tokens = tokenize(prompt)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Get model predictions\n",
    "        logits = model(tokens)\n",
    "        \n",
    "        # Get probabilities for next token\n",
    "        next_token_logits = logits[-1, :]  # Last token's predictions\n",
    "        next_token_probs = softmax(next_token_logits)\n",
    "        \n",
    "        # Sample next token\n",
    "        next_token = sample(next_token_probs)\n",
    "        \n",
    "        # Add to sequence\n",
    "        tokens.append(next_token)\n",
    "        \n",
    "        # Stop if end token\n",
    "        if next_token == END_TOKEN:\n",
    "            break\n",
    "    \n",
    "    return detokenize(tokens)\n",
    "```\n",
    "\n",
    "### GPT Evolution\n",
    "\n",
    "#### GPT-1 (2018)\n",
    "- 117M parameters\n",
    "- Demonstrated unsupervised pre-training effectiveness\n",
    "\n",
    "#### GPT-2 (2019)\n",
    "- 1.5B parameters\n",
    "- \"Too dangerous to release\" (initially)\n",
    "- Showed scaling benefits\n",
    "\n",
    "#### GPT-3 (2020)\n",
    "- 175B parameters\n",
    "- Few-shot learning via prompting\n",
    "- Emergent abilities\n",
    "\n",
    "```python\n",
    "# Few-shot prompting example\n",
    "prompt = \"\"\"\n",
    "Translate English to French:\n",
    "English: Hello\n",
    "French: Bonjour\n",
    "\n",
    "English: How are you?\n",
    "French: Comment allez-vous?\n",
    "\n",
    "English: Good morning\n",
    "French:\"\"\"\n",
    "\n",
    "# GPT-3 output: \"Bonjour\" (learned pattern from examples)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Interactive Exercises üéØ\n",
    "\n",
    "### Exercise 1: Embedding Similarity\n",
    "\n",
    "```python\n",
    "# Calculate cosine similarity between word vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude1 = np.linalg.norm(vec1)\n",
    "    magnitude2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "# Try with these example vectors:\n",
    "king = [0.2, 0.5, -0.1, 0.8, 0.3]\n",
    "queen = [0.1, 0.6, -0.2, 0.7, 0.4]\n",
    "apple = [0.8, -0.2, 0.9, 0.1, -0.3]\n",
    "\n",
    "print(f\"King-Queen similarity: {cosine_similarity(king, queen):.3f}\")\n",
    "print(f\"King-Apple similarity: {cosine_similarity(king, apple):.3f}\")\n",
    "```\n",
    "\n",
    "### Exercise 2: Attention Visualization\n",
    "\n",
    "```python\n",
    "# Visualize attention weights\n",
    "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "attention_weights = [\n",
    "    [0.1, 0.2, 0.1, 0.1, 0.4, 0.1],  # \"The\" attends to...\n",
    "    [0.2, 0.4, 0.2, 0.1, 0.05, 0.05], # \"cat\" attends to...\n",
    "    [0.1, 0.3, 0.3, 0.2, 0.05, 0.05], # \"sat\" attends to...\n",
    "    [0.1, 0.1, 0.6, 0.1, 0.05, 0.05], # \"on\" attends to...\n",
    "    [0.4, 0.1, 0.1, 0.1, 0.2, 0.1],   # \"the\" attends to...\n",
    "    [0.1, 0.4, 0.1, 0.2, 0.1, 0.1]    # \"mat\" attends to...\n",
    "]\n",
    "\n",
    "# Which word does \"cat\" pay most attention to?\n",
    "cat_attention = attention_weights[1]\n",
    "max_attention_idx = cat_attention.index(max(cat_attention))\n",
    "print(f\"'cat' pays most attention to: '{sentence[max_attention_idx]}'\")\n",
    "```\n",
    "\n",
    "### Exercise 3: Simple Language Model\n",
    "\n",
    "```python\n",
    "# Build a simple next-word predictor\n",
    "class SimpleLM:\n",
    "    def __init__(self):\n",
    "        self.word_counts = {}\n",
    "        self.context_counts = {}\n",
    "    \n",
    "    def train(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            for i in range(len(words) - 1):\n",
    "                context = words[i]\n",
    "                next_word = words[i + 1]\n",
    "                \n",
    "                if context not in self.word_counts:\n",
    "                    self.word_counts[context] = {}\n",
    "                    self.context_counts[context] = 0\n",
    "                \n",
    "                if next_word not in self.word_counts[context]:\n",
    "                    self.word_counts[context][next_word] = 0\n",
    "                \n",
    "                self.word_counts[context][next_word] += 1\n",
    "                self.context_counts[context] += 1\n",
    "    \n",
    "    def predict_next(self, context):\n",
    "        if context not in self.word_counts:\n",
    "            return \"unknown\"\n",
    "        \n",
    "        # Find most likely next word\n",
    "        best_word = max(self.word_counts[context].items(), \n",
    "                       key=lambda x: x[1])\n",
    "        return best_word[0]\n",
    "\n",
    "# Test it out!\n",
    "lm = SimpleLM()\n",
    "training_data = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog ran in the park\",\n",
    "    \"the cat slept on the bed\"\n",
    "]\n",
    "lm.train(training_data)\n",
    "print(f\"After 'the cat': {lm.predict_next('the cat')}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways üéØ\n",
    "\n",
    "### Evolution Timeline\n",
    "1. **Bag of Words** ‚Üí Sparse, no semantics\n",
    "2. **Word2Vec/GloVe** ‚Üí Dense embeddings, semantic relationships\n",
    "3. **RNNs/LSTMs** ‚Üí Sequential processing, memory\n",
    "4. **Attention** ‚Üí Focus mechanism, parallel processing\n",
    "5. **Transformers** ‚Üí Pure attention, scalability\n",
    "6. **BERT** ‚Üí Bidirectional understanding\n",
    "7. **GPT** ‚Üí Autoregressive generation, emergent abilities\n",
    "\n",
    "### Core Concepts Summary\n",
    "\n",
    "**Embeddings**: Convert words to dense vectors capturing semantics\n",
    "**Attention**: Mechanism to focus on relevant parts of input\n",
    "**Transformers**: Architecture based on self-attention and parallelization\n",
    "**Pre-training**: Learn general language understanding from large text corpora\n",
    "**Fine-tuning**: Adapt pre-trained models to specific tasks\n",
    "\n",
    "### Future Directions\n",
    "- **Scale**: Larger models, more parameters\n",
    "- **Efficiency**: Faster inference, smaller models\n",
    "- **Multimodality**: Text + images + audio\n",
    "- **Reasoning**: Better logical and mathematical reasoning\n",
    "- **Alignment**: Models that better understand human values and intentions\n",
    "\n",
    "---\n",
    "\n",
    "## Advanced Topics & Modern Developments üöÄ\n",
    "\n",
    "### 9. Transformer Variants & Optimizations\n",
    "\n",
    "#### RoPE (Rotary Position Embedding)\n",
    "Modern alternative to sinusoidal position encoding.\n",
    "\n",
    "```python\n",
    "def rope_embedding(q, k, position, d_model):\n",
    "    \"\"\"Rotary Position Embedding implementation\"\"\"\n",
    "    def rotate_half(x):\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        return np.concatenate([-x2, x1], axis=-1)\n",
    "    \n",
    "    # Create rotation matrix\n",
    "    inv_freq = 1.0 / (10000 ** (np.arange(0, d_model, 2) / d_model))\n",
    "    freqs = position * inv_freq\n",
    "    \n",
    "    cos_freqs = np.cos(freqs)\n",
    "    sin_freqs = np.sin(freqs)\n",
    "    \n",
    "    # Apply rotation\n",
    "    q_rotated = q * cos_freqs + rotate_half(q) * sin_freqs\n",
    "    k_rotated = k * cos_freqs + rotate_half(k) * sin_freqs\n",
    "    \n",
    "    return q_rotated, k_rotated\n",
    "```\n",
    "\n",
    "#### Flash Attention\n",
    "Memory-efficient attention computation.\n",
    "\n",
    "```python\n",
    "def flash_attention_concept(q, k, v, block_size=64):\n",
    "    \"\"\"\n",
    "    Conceptual Flash Attention - computes attention in blocks\n",
    "    to reduce memory usage from O(n¬≤) to O(n)\n",
    "    \"\"\"\n",
    "    seq_len, d_k = q.shape\n",
    "    output = np.zeros_like(v)\n",
    "    \n",
    "    # Process in blocks to save memory\n",
    "    for i in range(0, seq_len, block_size):\n",
    "        q_block = q[i:i+block_size]\n",
    "        \n",
    "        # Compute attention for this block\n",
    "        scores = q_block @ k.T / np.sqrt(d_k)\n",
    "        attn_weights = softmax(scores)\n",
    "        block_output = attn_weights @ v\n",
    "        \n",
    "        output[i:i+block_size] = block_output\n",
    "    \n",
    "    return output\n",
    "```\n",
    "\n",
    "### 10. Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Combine parametric knowledge (model weights) with non-parametric knowledge (external databases).\n",
    "\n",
    "```python\n",
    "class RAGSystem:\n",
    "    def __init__(self, retriever, generator):\n",
    "        self.retriever = retriever  # e.g., dense retrieval system\n",
    "        self.generator = generator  # e.g., GPT-like model\n",
    "    \n",
    "    def generate_with_retrieval(self, query, top_k=5):\n",
    "        # 1. Retrieve relevant documents\n",
    "        retrieved_docs = self.retriever.search(query, top_k=top_k)\n",
    "        \n",
    "        # 2. Create augmented prompt\n",
    "        context = \"\\n\".join([doc.text for doc in retrieved_docs])\n",
    "        augmented_prompt = f\"\"\"\n",
    "        Context: {context}\n",
    "        \n",
    "        Question: {query}\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        # 3. Generate response using retrieved context\n",
    "        response = self.generator.generate(augmented_prompt)\n",
    "        \n",
    "        return response, retrieved_docs\n",
    "\n",
    "# Example usage\n",
    "def dense_retrieval_example():\n",
    "    # Encode documents and queries using same embedding model\n",
    "    doc_embeddings = embed_documents(knowledge_base)\n",
    "    query_embedding = embed_query(\"What is photosynthesis?\")\n",
    "    \n",
    "    # Find most similar documents\n",
    "    similarities = cosine_similarity(query_embedding, doc_embeddings)\n",
    "    top_docs = get_top_k_documents(similarities, k=5)\n",
    "    \n",
    "    return top_docs\n",
    "```\n",
    "\n",
    "### 11. Parameter-Efficient Fine-tuning\n",
    "\n",
    "#### LoRA (Low-Rank Adaptation)\n",
    "Fine-tune large models by learning small adaptation matrices.\n",
    "\n",
    "```python\n",
    "class LoRALayer:\n",
    "    def __init__(self, original_layer, rank=16, alpha=32):\n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Low-rank matrices A and B\n",
    "        # W + ŒîW = W + BA where B ‚àà R^(d√ór), A ‚àà R^(r√ók)\n",
    "        d, k = original_layer.weight.shape\n",
    "        self.lora_A = np.random.randn(rank, k) * 0.01\n",
    "        self.lora_B = np.zeros((d, rank))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Original computation\n",
    "        original_output = self.original_layer(x)\n",
    "        \n",
    "        # LoRA adaptation\n",
    "        lora_output = x @ self.lora_A.T @ self.lora_B.T\n",
    "        \n",
    "        # Combine with scaling\n",
    "        return original_output + (self.alpha / self.rank) * lora_output\n",
    "```\n",
    "\n",
    "#### Adapters\n",
    "Insert small trainable modules between transformer layers.\n",
    "\n",
    "```python\n",
    "class AdapterLayer:\n",
    "    def __init__(self, hidden_size, adapter_size=64):\n",
    "        self.down_project = Linear(hidden_size, adapter_size)\n",
    "        self.up_project = Linear(adapter_size, hidden_size)\n",
    "        self.activation = ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Residual connection around adapter\n",
    "        adapter_output = self.up_project(\n",
    "            self.activation(self.down_project(x))\n",
    "        )\n",
    "        return x + adapter_output\n",
    "\n",
    "class TransformerWithAdapter(TransformerBlock):\n",
    "    def __init__(self, d_model, num_heads, d_ff, adapter_size=64):\n",
    "        super().__init__(d_model, num_heads, d_ff)\n",
    "        self.adapter = AdapterLayer(d_model, adapter_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Standard transformer computation\n",
    "        x = super().forward(x)\n",
    "        \n",
    "        # Apply adapter\n",
    "        x = self.adapter(x)\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "### 12. Instruction Tuning & RLHF\n",
    "\n",
    "#### Instruction Following\n",
    "Train models to follow human instructions accurately.\n",
    "\n",
    "```python\n",
    "# Instruction tuning dataset format\n",
    "instruction_examples = [\n",
    "    {\n",
    "        \"instruction\": \"Translate the following English text to Spanish\",\n",
    "        \"input\": \"Hello, how are you?\",\n",
    "        \"output\": \"Hola, ¬øc√≥mo est√°s?\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Summarize the following paragraph in one sentence\",\n",
    "        \"input\": \"Large language models have revolutionized natural language processing...\",\n",
    "        \"output\": \"Large language models have transformed NLP through their ability to understand and generate human-like text.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def format_instruction_prompt(instruction, input_text=\"\"):\n",
    "    if input_text:\n",
    "        return f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        return f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "```\n",
    "\n",
    "#### RLHF (Reinforcement Learning from Human Feedback)\n",
    "Align model outputs with human preferences.\n",
    "\n",
    "```python\n",
    "class RLHFTraining:\n",
    "    def __init__(self, policy_model, reward_model, reference_model):\n",
    "        self.policy_model = policy_model      # Model being trained\n",
    "        self.reward_model = reward_model      # Learned reward function\n",
    "        self.reference_model = reference_model # Original model (frozen)\n",
    "    \n",
    "    def compute_ppo_loss(self, prompts, responses):\n",
    "        # 1. Get log probabilities from policy and reference models\n",
    "        policy_logprobs = self.policy_model.get_log_probs(prompts, responses)\n",
    "        ref_logprobs = self.reference_model.get_log_probs(prompts, responses)\n",
    "        \n",
    "        # 2. Get rewards from reward model\n",
    "        rewards = self.reward_model.score(prompts, responses)\n",
    "        \n",
    "        # 3. Compute KL divergence penalty\n",
    "        kl_penalty = policy_logprobs - ref_logprobs\n",
    "        \n",
    "        # 4. PPO objective\n",
    "        ratio = torch.exp(policy_logprobs - ref_logprobs)\n",
    "        advantages = rewards - self.beta * kl_penalty\n",
    "        \n",
    "        # Clipped surrogate objective\n",
    "        clipped_ratio = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
    "        loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
    "        \n",
    "        return loss\n",
    "```\n",
    "\n",
    "### 13. Multimodal Models\n",
    "\n",
    "#### Vision-Language Models\n",
    "Combine text and image understanding.\n",
    "\n",
    "```python\n",
    "class VisionLanguageModel:\n",
    "    def __init__(self, vision_encoder, text_encoder, fusion_layer):\n",
    "        self.vision_encoder = vision_encoder  # e.g., Vision Transformer\n",
    "        self.text_encoder = text_encoder      # e.g., BERT/GPT\n",
    "        self.fusion_layer = fusion_layer      # Cross-attention\n",
    "    \n",
    "    def forward(self, image, text):\n",
    "        # Encode image patches\n",
    "        image_features = self.vision_encoder(image)  # Shape: (num_patches, d_model)\n",
    "        \n",
    "        # Encode text tokens\n",
    "        text_features = self.text_encoder(text)      # Shape: (seq_len, d_model)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        # Text attends to image features\n",
    "        fused_features = self.fusion_layer(\n",
    "            query=text_features,\n",
    "            key=image_features,\n",
    "            value=image_features\n",
    "        )\n",
    "        \n",
    "        return fused_features\n",
    "\n",
    "# Example: Image captioning\n",
    "def image_captioning_example():\n",
    "    model = VisionLanguageModel(vision_encoder, text_decoder, cross_attention)\n",
    "    \n",
    "    # Input image and start token\n",
    "    image = load_image(\"cat.jpg\")\n",
    "    caption_tokens = [\"<start>\"]\n",
    "    \n",
    "    # Generate caption autoregressively\n",
    "    for _ in range(max_caption_length):\n",
    "        features = model(image, caption_tokens)\n",
    "        next_token_probs = softmax(features[-1])  # Last token predictions\n",
    "        next_token = sample(next_token_probs)\n",
    "        \n",
    "        caption_tokens.append(next_token)\n",
    "        if next_token == \"<end>\":\n",
    "            break\n",
    "    \n",
    "    return \" \".join(caption_tokens[1:-1])  # Remove start/end tokens\n",
    "```\n",
    "\n",
    "### 14. Model Evaluation & Benchmarks\n",
    "\n",
    "#### Perplexity\n",
    "Measure how well model predicts test data.\n",
    "\n",
    "```python\n",
    "def calculate_perplexity(model, test_data):\n",
    "    total_log_likelihood = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for sentence in test_data:\n",
    "        tokens = tokenize(sentence)\n",
    "        \n",
    "        for i in range(1, len(tokens)):\n",
    "            context = tokens[:i]\n",
    "            target = tokens[i]\n",
    "            \n",
    "            # Get model prediction\n",
    "            logits = model(context)\n",
    "            log_prob = log_softmax(logits)[-1][target]\n",
    "            \n",
    "            total_log_likelihood += log_prob\n",
    "            total_tokens += 1\n",
    "    \n",
    "    # Perplexity = exp(-1/N * Œ£ log P(w_i))\n",
    "    avg_log_likelihood = total_log_likelihood / total_tokens\n",
    "    perplexity = math.exp(-avg_log_likelihood)\n",
    "    \n",
    "    return perplexity\n",
    "```\n",
    "\n",
    "#### BLEU Score\n",
    "Evaluate translation/generation quality.\n",
    "\n",
    "```python\n",
    "def calculate_bleu(reference, candidate, max_n=4):\n",
    "    \"\"\"Calculate BLEU score for text generation\"\"\"\n",
    "    def get_ngrams(tokens, n):\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    \n",
    "    def modified_precision(ref_tokens, cand_tokens, n):\n",
    "        ref_ngrams = Counter(get_ngrams(ref_tokens, n))\n",
    "        cand_ngrams = Counter(get_ngrams(cand_tokens, n))\n",
    "        \n",
    "        overlap = sum(min(cand_ngrams[ngram], ref_ngrams[ngram]) \n",
    "                     for ngram in cand_ngrams)\n",
    "        total = sum(cand_ngrams.values())\n",
    "        \n",
    "        return overlap / total if total > 0 else 0\n",
    "    \n",
    "    # Calculate precision for n-grams 1 to max_n\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        p = modified_precision(reference, candidate, n)\n",
    "        precisions.append(p)\n",
    "    \n",
    "    # Brevity penalty\n",
    "    ref_len = len(reference)\n",
    "    cand_len = len(candidate)\n",
    "    bp = min(1.0, math.exp(1 - ref_len / cand_len)) if cand_len > 0 else 0\n",
    "    \n",
    "    # BLEU score\n",
    "    if min(precisions) > 0:\n",
    "        bleu = bp * math.exp(sum(math.log(p) for p in precisions) / len(precisions))\n",
    "    else:\n",
    "        bleu = 0\n",
    "    \n",
    "    return bleu\n",
    "```\n",
    "\n",
    "### 15. Scaling Laws & Emergent Abilities\n",
    "\n",
    "#### Scaling Laws\n",
    "Relationship between model size, data, and performance.\n",
    "\n",
    "```python\n",
    "def scaling_law_prediction(N, D, C):\n",
    "    \"\"\"\n",
    "    Predict model performance based on scaling laws\n",
    "    N: Number of parameters\n",
    "    D: Dataset size\n",
    "    C: Compute budget\n",
    "    \"\"\"\n",
    "    # Simplified scaling law: Loss ‚àù N^(-Œ±) * D^(-Œ≤)\n",
    "    alpha = 0.076  # Parameter scaling exponent\n",
    "    beta = 0.095   # Data scaling exponent\n",
    "    \n",
    "    # Chinchilla scaling: optimal compute allocation\n",
    "    optimal_N = (C / 6) ** (1/2)  # Parameters\n",
    "    optimal_D = (C / 6) ** (1/2)  # Tokens\n",
    "    \n",
    "    predicted_loss = 1.0 / (N ** alpha * D ** beta)\n",
    "    \n",
    "    return predicted_loss, optimal_N, optimal_D\n",
    "```\n",
    "\n",
    "#### Emergent Abilities\n",
    "Capabilities that emerge at scale.\n",
    "\n",
    "```python\n",
    "# Examples of emergent abilities\n",
    "emergent_abilities = {\n",
    "    \"few_shot_learning\": {\n",
    "        \"threshold\": \"~10B parameters\",\n",
    "        \"description\": \"Learn from examples in context without gradient updates\"\n",
    "    },\n",
    "    \"chain_of_thought\": {\n",
    "        \"threshold\": \"~100B parameters\", \n",
    "        \"description\": \"Break down complex reasoning into steps\"\n",
    "    },\n",
    "    \"in_context_learning\": {\n",
    "        \"threshold\": \"~1B parameters\",\n",
    "        \"description\": \"Adapt to new tasks from examples in prompt\"\n",
    "    },\n",
    "    \"instruction_following\": {\n",
    "        \"threshold\": \"~10B parameters\",\n",
    "        \"description\": \"Follow complex, multi-step instructions\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def demonstrate_chain_of_thought():\n",
    "    prompt = \"\"\"\n",
    "    Question: A store has 12 apples. Sarah buys 3 apples, then John buys twice as many as Sarah. How many apples are left?\n",
    "    \n",
    "    Let me think step by step:\n",
    "    1. Store starts with 12 apples\n",
    "    2. Sarah buys 3 apples: 12 - 3 = 9 apples left\n",
    "    3. John buys twice as many as Sarah: 2 √ó 3 = 6 apples\n",
    "    4. After John's purchase: 9 - 6 = 3 apples left\n",
    "    \n",
    "    Answer: 3 apples\n",
    "    \n",
    "    Question: A library has 240 books. On Monday, 15 books were borrowed. On Tuesday, twice as many books were borrowed as on Monday. How many books remain?\n",
    "    \n",
    "    Let me think step by step:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "```\n",
    "\n",
    "### 16. Practical Implementation Tips\n",
    "\n",
    "#### Memory Optimization\n",
    "\n",
    "```python\n",
    "def gradient_checkpointing(model, inputs):\n",
    "    \"\"\"Trade compute for memory by recomputing activations during backward pass\"\"\"\n",
    "    \n",
    "    def checkpoint_function(layer, x):\n",
    "        # Don't store intermediate activations\n",
    "        with torch.no_grad():\n",
    "            y = layer(x)\n",
    "        \n",
    "        # Recompute during backward pass\n",
    "        y.requires_grad_(x.requires_grad)\n",
    "        return y\n",
    "    \n",
    "    x = inputs\n",
    "    for layer in model.layers:\n",
    "        x = checkpoint_function(layer, x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def mixed_precision_training():\n",
    "    \"\"\"Use FP16 for forward pass, FP32 for gradients\"\"\"\n",
    "    \n",
    "    # Automatic Mixed Precision (AMP) concept\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    with autocast():  # Use FP16\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Scale loss to prevent gradient underflow\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "```\n",
    "\n",
    "#### Efficient Inference\n",
    "\n",
    "```python\n",
    "class KVCache:\n",
    "    \"\"\"Cache key-value pairs for efficient autoregressive generation\"\"\"\n",
    "    \n",
    "    def __init__(self, max_seq_len, num_heads, head_dim):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.keys = np.zeros((self.max_seq_len, self.num_heads, self.head_dim))\n",
    "        self.values = np.zeros((self.max_seq_len, self.num_heads, self.head_dim))\n",
    "        self.current_length = 0\n",
    "    \n",
    "    def update(self, new_keys, new_values):\n",
    "        # Add new keys and values to cache\n",
    "        self.keys[self.current_length] = new_keys\n",
    "        self.values[self.current_length] = new_values\n",
    "        self.current_length += 1\n",
    "        \n",
    "        return (self.keys[:self.current_length], \n",
    "                self.values[:self.current_length])\n",
    "\n",
    "def efficient_generation_with_cache(model, prompt, max_length=100):\n",
    "    \"\"\"Generate text efficiently using KV caching\"\"\"\n",
    "    tokens = tokenize(prompt)\n",
    "    kv_cache = KVCache(max_length, model.num_heads, model.head_dim)\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        if i == 0:\n",
    "            # First pass: process entire prompt\n",
    "            logits = model(tokens, kv_cache=kv_cache)\n",
    "        else:\n",
    "            # Subsequent passes: only process last token\n",
    "            logits = model([tokens[-1]], kv_cache=kv_cache)\n",
    "        \n",
    "        # Sample next token\n",
    "        next_token = sample(softmax(logits[-1]))\n",
    "        tokens.append(next_token)\n",
    "        \n",
    "        if next_token == END_TOKEN:\n",
    "            break\n",
    "    \n",
    "    return detokenize(tokens)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Applications üåç\n",
    "\n",
    "### 1. Chatbots & Virtual Assistants\n",
    "```python\n",
    "class ConversationalAI:\n",
    "    def __init__(self, base_model, personality_prompt):\n",
    "        self.model = base_model\n",
    "        self.personality = personality_prompt\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def respond(self, user_input):\n",
    "        # Build context with personality and history\n",
    "        context = self.personality + \"\\n\\n\"\n",
    "        for turn in self.conversation_history[-5:]:  # Last 5 turns\n",
    "            context += f\"Human: {turn['user']}\\nAssistant: {turn['assistant']}\\n\\n\"\n",
    "        context += f\"Human: {user_input}\\nAssistant:\"\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.model.generate(context, max_length=200)\n",
    "        \n",
    "        # Update history\n",
    "        self.conversation_history.append({\n",
    "            \"user\": user_input,\n",
    "            \"assistant\": response\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "```\n",
    "\n",
    "### 2. Code Generation\n",
    "```python\n",
    "class CodeGenerator:\n",
    "    def __init__(self, code_model):\n",
    "        self.model = code_model\n",
    "    \n",
    "    def generate_function(self, description, language=\"python\"):\n",
    "        prompt = f\"\"\"\n",
    "        # Language: {language}\n",
    "        # Description: {description}\n",
    "        \n",
    "        def \"\"\"\n",
    "        \n",
    "        code = self.model.generate(prompt, \n",
    "                                 stop_tokens=[\"def \", \"class \", \"\\n\\n\"],\n",
    "                                 temperature=0.2)  # Lower temp for code\n",
    "        \n",
    "        return code\n",
    "    \n",
    "    def explain_code(self, code):\n",
    "        prompt = f\"\"\"\n",
    "        Explain this code step by step:\n",
    "        \n",
    "        ```\n",
    "        {code}\n",
    "        ```\n",
    "        \n",
    "        Explanation:\"\"\"\n",
    "        \n",
    "        explanation = self.model.generate(prompt, max_length=500)\n",
    "        return explanation\n",
    "```\n",
    "\n",
    "### 3. Content Creation\n",
    "```python\n",
    "class ContentCreator:\n",
    "    def __init__(self, creative_model):\n",
    "        self.model = creative_model\n",
    "    \n",
    "    def write_blog_post(self, topic, tone=\"professional\", length=\"medium\"):\n",
    "        prompt = f\"\"\"\n",
    "        Write a {tone} blog post about {topic}.\n",
    "        Length: {length}\n",
    "        \n",
    "        Title: \"\"\"\n",
    "        \n",
    "        blog_post = self.model.generate(prompt, \n",
    "                                      temperature=0.7,  # Higher temp for creativity\n",
    "                                      max_length=1000)\n",
    "        return blog_post\n",
    "    \n",
    "    def generate_marketing_copy(self, product, audience):\n",
    "        prompt = f\"\"\"\n",
    "        Create compelling marketing copy for {product}.\n",
    "        Target audience: {audience}\n",
    "        \n",
    "        Headlines (3 options):\n",
    "        1.\"\"\"\n",
    "        \n",
    "        copy = self.model.generate(prompt, temperature=0.8)\n",
    "        return copy\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Optimization Checklist ‚úÖ\n",
    "\n",
    "### Training Optimization\n",
    "- [ ] **Gradient Accumulation**: Simulate larger batch sizes\n",
    "- [ ] **Mixed Precision**: Use FP16 for memory efficiency  \n",
    "- [ ] **Gradient Clipping**: Prevent exploding gradients\n",
    "- [ ] **Learning Rate Scheduling**: Warmup + decay strategies\n",
    "- [ ] **Data Loading**: Async data loading and preprocessing\n",
    "- [ ] **Model Parallelism**: Split model across GPUs for large models\n",
    "- [ ] **Data Parallelism**: Split batches across GPUs\n",
    "\n",
    "### Inference Optimization  \n",
    "- [ ] **KV Caching**: Cache attention computations\n",
    "- [ ] **Batching**: Process multiple requests together\n",
    "- [ ] **Quantization**: Reduce model precision (INT8, INT4)\n",
    "- [ ] **Pruning**: Remove unimportant connections\n",
    "- [ ] **Distillation**: Train smaller student models\n",
    "- [ ] **Speculative Decoding**: Parallel generation strategies\n",
    "\n",
    "### Memory Management\n",
    "- [ ] **Gradient Checkpointing**: Trade compute for memory\n",
    "- [ ] **Parameter Sharding**: Distribute model parameters\n",
    "- [ ] **Activation Checkpointing**: Don't store all activations\n",
    "- [ ] **Dynamic Batching**: Adjust batch size based on sequence length\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion: The Journey Continues üéØ\n",
    "\n",
    "You've now explored the fascinating world of NLP and Large Language Models! From simple word vectors to sophisticated transformers, you've learned:\n",
    "\n",
    "### Core Transformations\n",
    "**Text ‚Üí Numbers ‚Üí Understanding ‚Üí Generation**\n",
    "\n",
    "### Key Breakthroughs\n",
    "1. **Word2Vec**: Semantic word representations\n",
    "2. **Attention**: Focus mechanisms\n",
    "3. **Transformers**: Parallelizable sequence processing  \n",
    "4. **Pre-training**: Learn from massive text corpora\n",
    "5. **Scale**: Bigger models, emergent abilities\n",
    "\n",
    "### What's Next?\n",
    "- **Experiment**: Try implementing these concepts\n",
    "- **Practice**: Work on real NLP projects\n",
    "- **Stay Updated**: Follow latest research (arXiv, conferences)\n",
    "- **Build**: Create your own language models\n",
    "- **Ethics**: Consider responsible AI development\n",
    "\n",
    "### Resources for Continued Learning\n",
    "- **Papers**: \"Attention Is All You Need\", \"BERT\", \"GPT\" series\n",
    "- **Courses**: Stanford CS224N, fast.ai NLP\n",
    "- **Libraries**: Transformers (HuggingFace), PyTorch, TensorFlow\n",
    "- **Datasets**: Common Crawl, BookCorpus, OpenWebText\n",
    "\n",
    "### Final Thought\n",
    "The field of NLP is rapidly evolving. What seems impossible today might be commonplace tomorrow. Keep learning, keep building, and most importantly, keep wondering about the beautiful complexity of human language and how we can teach machines to understand it.\n",
    "\n",
    "Happy coding! üöÄ‚ú®\n",
    "\n",
    "---\n",
    "\n",
    "*\"The limits of my language are the limits of my world.\" - Ludwig Wittgenstein*\n",
    "\n",
    "*In the age of AI, we're expanding both our language and our world.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
