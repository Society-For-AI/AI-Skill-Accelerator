{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5edfb6f7-fd77-44ac-9848-5b34845b9188",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ðŸ“„ Introduction: Hugging Face Applications\n",
    "\n",
    "This notebook explores the Hugging Face ecosystem, focusing on its powerful **Transformers** library and demonstrating how to apply pre-trained models for tasks like text generation and question answering.\n",
    "\n",
    "Hugging Face is an open-source platform built around transformer-based models for natural language processing (NLP) and multimodal AI, covering text, vision, and audio. It serves as a hub for:\n",
    "\n",
    "* **Thousands of pre-trained models** in a centralized Model Hub\n",
    "* **The `transformers` library** for easy access to state-of-the-art models\n",
    "* **High-level APIs** for training, inference, and deployment\n",
    "* **Multi-framework support** for `PyTorch`, `TensorFlow`, and `JAX`\n",
    "* **Auxiliary tools** like `datasets`, `evaluate`, and `accelerate` to streamline training pipelines\n",
    "* **Deployment services** including Hosted Inference Endpoints, AutoTrain, and Spaces\n",
    "\n",
    "In short, think of Hugging Face as the **â€œpip + GitHub + Docker Hubâ€** of modern machine learningâ€”especially for NLP.\n",
    "\n",
    "### ðŸ”§ Core Library: `transformers`\n",
    "\n",
    "The `transformers` library offers a **unified API** for thousands of models such as BERT, GPT-2/3/4, T5, BLOOM, and LLaMA, with seamless task switching for classification, summarization, question answering, text generation, translation, and more. Its key abstractionsâ€”`AutoModel`, `AutoTokenizer`, and `pipeline()`â€”make loading models, tokenizing input, and running inference simple and intuitive.\n",
    "\n",
    "**`datasets`**\n",
    "\n",
    "* Efficient preprocessing and loading of large datasets with streaming and lazy loading\n",
    "* Supports Hugging Face datasets, custom formats, and integration with common ML frameworks\n",
    "\n",
    "**`accelerate`**\n",
    "\n",
    "* Simplifies multi-GPU and distributed training\n",
    "* Scales training workloads with minimal code changes\n",
    "\n",
    "**`gradio` / Spaces**\n",
    "\n",
    "* Rapid prototyping of interactive UIs for models\n",
    "* Ideal for public demos, internal tools, and proof-of-concepts\n",
    "\n",
    "\n",
    "By specifying a task and optionally a model, we can tap into state-of-the-art capabilities with minimal code, making Hugging Face an essential toolkit for modern AI development.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cad803-e81b-476e-8adf-10b1187c7adc",
   "metadata": {},
   "source": [
    "| Use Case                      | Tooling / Method                               |\n",
    "| ----------------------------- | ---------------------------------------------- |\n",
    "| Text classification           | `pipeline(\"text-classification\")`              |\n",
    "| Named entity recognition      | `pipeline(\"ner\")`                              |\n",
    "| Conversational AI / Chatbot   | `pipeline(\"conversational\")` + RAG             |\n",
    "| Document summarization        | `pipeline(\"summarization\")` or fine-tuned BART |\n",
    "| Question answering            | `pipeline(\"question-answering\")`               |\n",
    "| Custom training (e.g., GPT)   | `Trainer` / `accelerate` + your dataset        |\n",
    "| Multimodal input (image+text) | `transformers` + CLIP/BLIP/GIT models          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defaa407-d586-422a-abe2-591fe0f0ae49",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Deployment Options\n",
    "\n",
    "* **On-device** (quantized models via ONNX or TFLite)\n",
    "* **Server-side** (Hugging Face Inference API, TorchServe, Triton, or FastAPI)\n",
    "* **Serverless** (AWS SageMaker integration via `huggingface_hub` SDK)\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Hugging Face\n",
    "\n",
    "* When you want **quick access to SOTA NLP models**\n",
    "* When you need to **fine-tune** on domain-specific data\n",
    "* When building **LLM-powered apps**, especially with **LangChain** or **Haystack**\n",
    "* When evaluating models for **research, experimentation, or benchmarking**\n",
    "* When you need **multi-modal ML** (vision-language, speech-text)\n",
    "\n",
    "---\n",
    "### Are GPT, Claude, and Sonnet Hugging Face models?\n",
    "---\n",
    "\n",
    "| Model Name                    | Creator         | Hosted on Hugging Face? | Open Source?           | Can You Use via HF Transformers?     |\n",
    "| ----------------------------- | --------------- | ----------------------- | ---------------------- | ------------------------------------ |\n",
    "| **GPT-4 / GPT-3.5**           | OpenAI          | âŒ (closed)              | âŒ No                   | ðŸ”„ Only via OpenAI API, not directly |\n",
    "| **Claude 2 / 3 (Sonnet)**     | Anthropic       | âŒ (closed)              | âŒ No                   | ðŸ”„ Only via Anthropic API            |\n",
    "| **LLaMA / LLaMA 2 / LLaMA 3** | Meta            | âœ… (with license)        | ðŸ”“ Yes (some variants) | âœ… Yes                                |\n",
    "| **Mistral / Mixtral**         | Mistral AI      | âœ…                       | âœ… Yes                  | âœ… Yes                                |\n",
    "| **Gemma**                     | Google DeepMind | âœ…                       | âœ… Yes                  | âœ… Yes                                |\n",
    "| **Falcon**                    | TII (UAE)       | âœ…                       | âœ… Yes                  | âœ… Yes                                |\n",
    "| **Command R / R+**            | Cohere          | âœ… (some versions)       | âœ… (R+ is open)         | âœ… Yes                                |\n",
    "| **BLOOM**                     | BigScience      | âœ… (hosted by HF itself) | âœ… Yes                  | âœ… Yes                                |\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "> **Hugging Face is the de facto standard for working with transformer models.**\n",
    "> It abstracts complexity while remaining flexible, production-friendly, and deeply extensible.\n",
    "\n",
    "You can start with `pip install transformers`, and within minutes youâ€™re running models that power real-world products at Meta, Google, and OpenAI.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4bc5b8-e960-4090-972e-741b57414120",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (4.55.0)\n",
      "Requirement already satisfied: torch in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from transformers) (0.6.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from triton==3.4.0->torch) (80.9.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from triton==3.4.0->torch) (8.7.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from importlib-metadata->triton==3.4.0->torch) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages (from requests->transformers) (2025.8.3)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.1.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5990f966-fe3f-4cbf-8f9c-6a92bc71568c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1) Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dabfcb59-560e-4a84-9aa5-795c71a9830b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wizard says: \"You don't have to worry about that. I'm going to tell you what I think about it. I think that this is the first\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "wizard = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "response = wizard(\"A wizard says:\", max_new_tokens=29)\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de63449a-5860-40ab-a80f-620f5bad086d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Whatâ€™s Happening:\n",
    "\n",
    "- Task: \"text-generation\" tells Hugging Face to generate text based on a prompt.\n",
    "\n",
    "- Model: \"gpt2\" is a small, general-purpose language model trained by OpenAI.\n",
    "\n",
    "- Process:\n",
    "\n",
    "    - The pipeline loads the model and tokenizer.\n",
    "    \n",
    "    - The input prompt \"A wizard says:\" is tokenized into numerical IDs.\n",
    "    \n",
    "    - GPT-2 predicts the next sequence of tokens until the max_new_tokens limit is reached.\n",
    "\n",
    "- Output: A coherent text continuation generated by GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d922289-ed89-4cb8-a2db-58b3edb7be54",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2)  Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc6d8c1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iamthevectorian/.local/share/virtualenvs/AI-Skill-Accelerator-NLP-hF9l7mPg/lib/python3.9/site-packages/transformers/pipelines/question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "question_answer_pipline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "question_answer = question_answer_pipline({'context' : \"Ade goes to school, he is in primary 5. He loves rice.\", \n",
    "                         'question': \"What is the name of the person in our story?\"})\n",
    "answer = question_answer['answer']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3f398b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Whatâ€™s Happening:\n",
    "\n",
    "- Task: \"question-answering\" extracts an answer from a given context.\n",
    "\n",
    "- Model: \"distilbert-base-cased-distilled-squad\" is a smaller, faster variant of BERT fine-tuned on the SQuAD dataset for span extraction.\n",
    "\n",
    "- Process:\n",
    "\n",
    "    - Both the context and question are tokenized.\n",
    "    \n",
    "    - The model identifies the most probable start and end positions of the answer within the context.\n",
    "\n",
    "- Output: The substring from the context that best answers the question (in this case, \"Ade\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ba16a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 3) Finish My Story, Please!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a189b4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Ade goes to school, he is in primary 5. He loves rice and iced tea and he really enjoys the whole family. He's not really a fan of the restaurant so he's not going to go to any. So\"}]\n"
     ]
    }
   ],
   "source": [
    "story_pipline = pipeline(\"text-generation\")\n",
    "story = story_pipline(\"Ade goes to school, he is in primary 5. He loves rice and \", max_new_tokens=30)\n",
    "\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7310498",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Practice answering question on completed story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d6f1a3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "question_answer = question_answer_pipline({'context' : f\"{story}\", \n",
    "                         'question': \"Ade is tight with what?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3e6d510",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rice and iced tea\n"
     ]
    }
   ],
   "source": [
    "answer = question_answer['answer']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1de2e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 4) Detect the Mood of a Message (Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95e4e266",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998644590377808}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "print(sentiment(\"I had the best day ever!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70fe8d9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Key Points\n",
    "- The pipeline() API in Hugging Face allows rapid prototyping without deep knowledge of model architecture.\n",
    "\n",
    "- Default models are used if none is specified, but explicitly setting the model ensures reproducibility\n",
    "\n",
    "     **Examples of Defaults**\n",
    "    \n",
    "    | Task                           | Code Example                       | Default Model                                     |\n",
    "    | ------------------------------ | ---------------------------------- | ------------------------------------------------- |\n",
    "    | **Sentiment Analysis**         | `pipeline(\"sentiment-analysis\")`   | `distilbert-base-uncased-finetuned-sst-2-english` |\n",
    "    | **Text Generation**            | `pipeline(\"text-generation\")`      | `gpt2`                                            |\n",
    "    | **Question Answering**         | `pipeline(\"question-answering\")`   | `distilbert-base-cased-distilled-squad`           |\n",
    "    | **Translation**                | `pipeline(\"translation_en_to_fr\")` | `Helsinki-NLP/opus-mt-en-fr`                      |\n",
    "    | **Summarization**              | `pipeline(\"summarization\")`        | `facebook/bart-large-cnn`                         |\n",
    "    | **NER (Token Classification)** | `pipeline(\"ner\")`                  | `dslim/bert-base-NER`                             |\n",
    "    | **Image Classification**       | `pipeline(\"image-classification\")` | `google/vit-base-patch16-224`                     |\n",
    "\n",
    "\n",
    "- Both examples run entirely on CPU here, but the same code can be accelerated using GPUs for faster inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47a01e-5642-4821-a961-d1fa4ed9728d",
   "metadata": {},
   "source": [
    "#### Whatâ€™s Happening:\n",
    "\n",
    "- Task: \"question-answering\" extracts an answer from a given context.\n",
    "\n",
    "- Model: \"distilbert-base-cased-distilled-squad\" is a smaller, faster variant of BERT fine-tuned on the SQuAD dataset for span extraction.\n",
    "\n",
    "- Process:\n",
    "\n",
    "    - Both the context and question are tokenized.\n",
    "    \n",
    "    - The model identifies the most probable start and end positions of the answer within the context.\n",
    "\n",
    "- Output: The substring from the context that best answers the question (in this case, \"Ade\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3) Finish My Story, Please!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "story_pipline = pipeline(\"text-generation\")\n",
    "story = story_pipline(\"Ade goes to school, he is in primary 5. He loves rice and \", max_new_tokens=30)\n",
    "\n",
    "print(story)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Practice answering question on completed story"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question_answer = question_answer_pipline({'context' : f\"{story}\", \n",
    "                         'question': \"Ade is tight with what?\"})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "answer = question_answer['answer']\n",
    "print(answer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4) Detect the Mood of a Message (Sentiment Analysis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "print(sentiment(\"I had the best day ever!\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Key Points\n",
    "- The pipeline() API in Hugging Face allows rapid prototyping without deep knowledge of model architecture.\n",
    "\n",
    "- Default models are used if none is specified, but explicitly setting the model ensures reproducibility\n",
    "\n",
    "     **Examples of Defaults**\n",
    "    \n",
    "    | Task                           | Code Example                       | Default Model                                     |\n",
    "    | ------------------------------ | ---------------------------------- | ------------------------------------------------- |\n",
    "    | **Sentiment Analysis**         | `pipeline(\"sentiment-analysis\")`   | `distilbert-base-uncased-finetuned-sst-2-english` |\n",
    "    | **Text Generation**            | `pipeline(\"text-generation\")`      | `gpt2`                                            |\n",
    "    | **Question Answering**         | `pipeline(\"question-answering\")`   | `distilbert-base-cased-distilled-squad`           |\n",
    "    | **Translation**                | `pipeline(\"translation_en_to_fr\")` | `Helsinki-NLP/opus-mt-en-fr`                      |\n",
    "    | **Summarization**              | `pipeline(\"summarization\")`        | `facebook/bart-large-cnn`                         |\n",
    "    | **NER (Token Classification)** | `pipeline(\"ner\")`                  | `dslim/bert-base-NER`                             |\n",
    "    | **Image Classification**       | `pipeline(\"image-classification\")` | `google/vit-base-patch16-224`                     |\n",
    "\n",
    "\n",
    "- Both examples run entirely on CPU here, but the same code can be accelerated using GPUs for faster inference."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Practice Challenge: Translation Task\n",
    "\n",
    "**Objective:**\n",
    "Use Hugging Faceâ€™s `pipeline()` to translate a sentence from English to French.\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a translation pipeline using the `\"translation_en_to_fr\"` task.\n",
    "2. Use the **Helsinki-NLP/opus-mt-en-fr** model (a popular open-source translation model).\n",
    "3. Translate the sentence:\n",
    "\n",
    "   > `\"Artificial intelligence is transforming the world of technology.\"`\n",
    "4. Print the translated text.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "0f0ea31e-0e20-4411-b459-8e8e6f67a6ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03333d94-e01e-4e1f-9e8d-0967fb9708f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Ade goes to school, he is in primary 5. He loves rice and iced tea and he loves being in his own home. He is a really smart guy.\\n\\nAnd, if he has a problem with the school'}]\n"
     ]
    }
   ],
   "source": [
    "story_pipline = pipeline(\"text-generation\")\n",
    "story = story_pipline(\"Ade goes to school, he is in primary 5. He loves rice and \", max_new_tokens=30)\n",
    "\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ef67e1-e3cb-40bb-b320-8710601c7531",
   "metadata": {},
   "source": [
    "### Practice answering question on completed story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9aaa58c-1d7e-424b-b3d0-e00c22e1429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer = question_answer_pipline({'context' : f\"{story}\", \n",
    "                         'question': \"Ade is tight with what?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aaa260d-7f4a-4016-b545-a07c50d2edd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rice and iced tea\n"
     ]
    }
   ],
   "source": [
    "answer = question_answer['answer']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16865e7-04f7-4d0e-a784-be7c159f5a22",
   "metadata": {},
   "source": [
    "### 4) Detect the Mood of a Message (Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b03685fa-8551-4c6f-98f7-37e01bac512f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998644590377808}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "print(sentiment(\"I had the best day ever!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb9662-a8d7-4b7b-8761-05dcc32bff59",
   "metadata": {},
   "source": [
    "#### Key Points\n",
    "- The pipeline() API in Hugging Face allows rapid prototyping without deep knowledge of model architecture.\n",
    "\n",
    "- Default models are used if none is specified, but explicitly setting the model ensures reproducibility\n",
    "\n",
    "     **Examples of Defaults**\n",
    "    \n",
    "    | Task                           | Code Example                       | Default Model                                     |\n",
    "    | ------------------------------ | ---------------------------------- | ------------------------------------------------- |\n",
    "    | **Sentiment Analysis**         | `pipeline(\"sentiment-analysis\")`   | `distilbert-base-uncased-finetuned-sst-2-english` |\n",
    "    | **Text Generation**            | `pipeline(\"text-generation\")`      | `gpt2`                                            |\n",
    "    | **Question Answering**         | `pipeline(\"question-answering\")`   | `distilbert-base-cased-distilled-squad`           |\n",
    "    | **Translation**                | `pipeline(\"translation_en_to_fr\")` | `Helsinki-NLP/opus-mt-en-fr`                      |\n",
    "    | **Summarization**              | `pipeline(\"summarization\")`        | `facebook/bart-large-cnn`                         |\n",
    "    | **NER (Token Classification)** | `pipeline(\"ner\")`                  | `dslim/bert-base-NER`                             |\n",
    "    | **Image Classification**       | `pipeline(\"image-classification\")` | `google/vit-base-patch16-224`                     |\n",
    "\n",
    "\n",
    "- Both examples run entirely on CPU here, but the same code can be accelerated using GPUs for faster inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147f953-2a28-49a7-b0c4-36f53217c667",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practice Challenge: Translation Task\n",
    "\n",
    "**Objective:**\n",
    "Use Hugging Faceâ€™s `pipeline()` to translate a sentence from English to French.\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Create a translation pipeline using the `\"translation_en_to_fr\"` task.\n",
    "2. Use the **Helsinki-NLP/opus-mt-en-fr** model (a popular open-source translation model).\n",
    "3. Translate the sentence:\n",
    "\n",
    "   > `\"Artificial intelligence is transforming the world of technology.\"`\n",
    "4. Print the translated text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6bcd36-a0cb-4858-a42f-cf6cddea01d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32d3c5-63b8-4465-bfb0-dc454f5c3a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e9db2-7045-442b-bee3-309bb6d1a30a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}