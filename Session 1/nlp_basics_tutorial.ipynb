{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Natural Language Processing (NLP) Fundamentals & Text Preprocessing\n",
    "\n",
    "**Author:** Chigozilai Kejeh  \n",
    "**Connect:** [Linkedin](https://www.linkedin.com/in/chigozilai-kejeh-058014143/)  \n",
    "**Level:** Beginner to Intermediate  \n",
    "**Duration:** 1-2 hours  \n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "- Understand what NLP is and its core concepts\n",
    "- Master text preprocessing fundamentals\n",
    "- Work with NLTK and spaCy libraries\n",
    "- Implement tokenization, stemming, and POS tagging\n",
    "- Build a complete sentiment analysis model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup & Installation\n",
    "\n",
    "First, let's install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install nltk spacy scikit-learn matplotlib seaborn wordcloud\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Import libraries\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ What is Natural Language Processing (NLP)?\n",
    "\n",
    "## üìñ Theory Overview\n",
    "\n",
    "**Natural Language Processing (NLP)** is a branch of artificial intelligence that helps computers understand, interpret, and generate human language in a valuable way.\n",
    "\n",
    "### üß† Core NLP Concepts:\n",
    "\n",
    "```\n",
    "NLP Pipeline:\n",
    "Raw Text ‚Üí Preprocessing ‚Üí Feature Extraction ‚Üí Model ‚Üí Results\n",
    "     ‚Üì           ‚Üì              ‚Üì            ‚Üì        ‚Üì\n",
    "\"I love AI\"  ‚Üí Clean text  ‚Üí Vectors    ‚Üí Algorithm ‚Üí Positive\n",
    "```\n",
    "\n",
    "### üéØ NLP Applications:\n",
    "- **Sentiment Analysis**: Understanding emotions in text\n",
    "- **Machine Translation**: Google Translate\n",
    "- **Chatbots**: Customer service automation\n",
    "- **Text Summarization**: News article summaries\n",
    "- **Named Entity Recognition**: Extracting names, places, organizations\n",
    "\n",
    "### üìö Further Reading:\n",
    "- [NLP Courses - Kaggle](https://www.kaggle.com/discussions/questions-and-answers/278543)\n",
    "- [LLM Course - Hugging Face](https://huggingface.co/learn/llm-course/chapter1/1)\n",
    "- [NLP Tutorial - GeeksforGeeks](https://www.geeksforgeeks.org/nlp/natural-language-processing-nlp-tutorial/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Hands-On Practice: Your First NLP Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a simple example\n",
    "sample_text = \"Natural Language Processing is amazing! I love learning about AI and machine learning.\"\n",
    "\n",
    "# Basic text analysis\n",
    "print(\"üìù Original Text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nüìä Basic Statistics:\")\n",
    "print(f\"Character count: {len(sample_text)}\")\n",
    "print(f\"Word count: {len(sample_text.split())}\")\n",
    "print(f\"Sentence count: {sample_text.count('.')}\")\n",
    "\n",
    "# Case conversion examples\n",
    "print(\"\\nüîÑ Text Transformations:\")\n",
    "print(f\"Lowercase: {sample_text.lower()}\")\n",
    "print(f\"Uppercase: {sample_text.upper()}\")\n",
    "print(f\"Title Case: {sample_text.title()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Text Preprocessing Fundamentals\n",
    "\n",
    "## üìñ Theory Overview\n",
    "\n",
    "**Text Preprocessing** is the crucial first step in any NLP pipeline. Raw text is messy and needs cleaning before analysis. No matter how advanced the model, success still begins with clean, consistent input so you can reap reliable, high-quality results on the other side.\n",
    "\n",
    "![Local Image](img/nlp_pipeline.png)\n",
    "\n",
    "### üßπ Common Preprocessing Steps:\n",
    "\n",
    "```\n",
    "Raw Text: \"Hello World! I'm learning NLP... It's GREAT!!!\"\n",
    "    ‚Üì\n",
    "Lowercase: \"hello world! i'm learning nlp... it's great!!!\"\n",
    "    ‚Üì\n",
    "Remove Punctuation: \"hello world im learning nlp its great\"\n",
    "    ‚Üì\n",
    "Tokenization: [\"hello\", \"world\", \"im\", \"learning\", \"nlp\", \"its\", \"great\"]\n",
    "    ‚Üì\n",
    "Remove Stopwords: [\"learning\", \"nlp\", \"great\"]\n",
    "    ‚Üì\n",
    "Stemming: [\"learn\", \"nlp\", \"great\"]\n",
    "```\n",
    "\n",
    "### üìö Further Reading:\n",
    "- [Text Preprocessing Guide](https://www.geeksforgeeks.org/machine-learning/text-preprocessing-in-python-set-1/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Hands-On Practice: Text Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Sample messy text\n",
    "messy_text = \"\"\"\n",
    "    Hello World!!! I'm learning NLP... It's ABSOLUTELY amazing!!! \n",
    "    Visit https://example.com for more info. Email: test@email.com\n",
    "    Phone: +1-234-567-8900. #NLP #MachineLearning @AIExpert\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Original Messy Text:\")\n",
    "print(repr(messy_text))\n",
    "\n",
    "def clean_text_step_by_step(text):\n",
    "    \"\"\"Demonstrate text cleaning step by step\"\"\"\n",
    "    \n",
    "    print(\"\\nüßπ Step-by-Step Text Cleaning:\")\n",
    "    \n",
    "    # Step 1: Remove extra whitespace\n",
    "    step1 = re.sub(r'\\s+', ' ', text.strip())\n",
    "    print(f\"\\n1. Remove extra whitespace:\\n{repr(step1)}\")\n",
    "    \n",
    "    # Step 2: Convert to lowercase\n",
    "    step2 = step1.lower()\n",
    "    print(f\"\\n2. Convert to lowercase:\\n{step2}\")\n",
    "    \n",
    "    # Step 3: Remove URLs\n",
    "    step3 = re.sub(r'http\\S+|www\\S+|https\\S+', '', step2, flags=re.MULTILINE)\n",
    "    print(f\"\\n3. Remove URLs:\\n{step3}\")\n",
    "    \n",
    "    # Step 4: Remove email addresses\n",
    "    step4 = re.sub(r'\\S+@\\S+', '', step3)\n",
    "    print(f\"\\n4. Remove emails:\\n{step4}\")\n",
    "    \n",
    "    # Step 5: Remove phone numbers\n",
    "    step5 = re.sub(r'\\+?\\d[\\d\\s\\-\\(\\)]+\\d', '', step4)\n",
    "    print(f\"\\n5. Remove phone numbers:\\n{step5}\")\n",
    "    \n",
    "    # Step 6: Remove social media mentions and hashtags\n",
    "    step6 = re.sub(r'[@#]\\w+', '', step5)\n",
    "    print(f\"\\n6. Remove social media tags:\\n{step6}\")\n",
    "    \n",
    "    # Step 7: Remove punctuation\n",
    "    step7 = step6.translate(str.maketrans('', '', string.punctuation))\n",
    "    print(f\"\\n7. Remove punctuation:\\n{step7}\")\n",
    "    \n",
    "    # Step 8: Remove extra spaces again\n",
    "    final = re.sub(r'\\s+', ' ', step7.strip())\n",
    "    print(f\"\\n8. Final cleaned text:\\n{final}\")\n",
    "    \n",
    "    return final\n",
    "\n",
    "cleaned_text = clean_text_step_by_step(messy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ NLTK Library Deep Dive\n",
    "\n",
    "## üìñ Theory Overview\n",
    "\n",
    "**NLTK (Natural Language Toolkit)** is one of the most popular Python libraries for NLP. It provides easy-to-use interfaces to over 50 corpora and lexical resources.\n",
    "\n",
    "### üîß NLTK Key Features:\n",
    "- **Tokenization**: Breaking text into words/sentences\n",
    "- **Stemming & Lemmatization**: Reducing words to root forms\n",
    "- **POS Tagging**: Identifying parts of speech\n",
    "- **Named Entity Recognition**: Extracting entities\n",
    "- **Sentiment Analysis**: VADER sentiment analyzer\n",
    "\n",
    "### üìö Further Reading:\n",
    "- [NLTK Official Documentation](https://www.nltk.org/)\n",
    "- [NLTK Book Online](https://www.nltk.org/book/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Hands-On Practice: Tokenization\n",
    "\n",
    "Tokenization is splitting raw text into smaller units called tokens‚Äîthese might be words, subwords, or individual characters.\n",
    "\n",
    "![tokenization](img/token.png)\n",
    "\n",
    "Humans can intuitively parse the text into `generative AI is fascinating and is the future`, but machines require explicit instructions to recognize word boundaries. Tokenization bridges this gap, enabling machines to identify and separate individual words or meaningful subunits within the text.\n",
    "\n",
    "Let‚Äôs implement a basic word tokenizer using Python. This example will split a sentence into words and punctuation marks, demonstrating how tokenization structures raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Generative', 'AI', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "def simple_tokenize(text):\n",
    "    tokens = []\n",
    "    current_word = \"\"\n",
    "    for char in text:\n",
    "        if char.isalnum():\n",
    "            current_word += char\n",
    "        else:\n",
    "            if current_word != \"\":\n",
    "                tokens.append(current_word)  # Append the accumulated word.\n",
    "                current_word = \"\"\n",
    "            if char.strip() != \"\":  # Ignore whitespace.\n",
    "                tokens.append(char)  # Append punctuation or other non-alphanumeric characters.\n",
    "    if current_word != \"\":\n",
    "        tokens.append(current_word)  # Append any remaining word.\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Generative AI is fascinating!\"\n",
    "tokens = simple_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Sample text for tokenization\n",
    "text = \"\"\"\n",
    "Natural Language Processing is a fascinating field! It combines linguistics, \n",
    "computer science, and artificial intelligence. NLP helps computers understand \n",
    "human language. Isn't that amazing?\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Original Text:\")\n",
    "print(text.strip())\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(f\"\\nüìÑ Sentence Tokenization ({len(sentences)} sentences):\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sentence.strip()}\")\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(f\"\\nüî§ Word Tokenization ({len(words)} tokens):\")\n",
    "print(words)\n",
    "\n",
    "# Custom tokenization (only alphabetic words)\n",
    "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "clean_words = regexp_tokenizer.tokenize(text.lower())\n",
    "print(f\"\\nüßπ Clean Word Tokens ({len(clean_words)} tokens):\")\n",
    "print(clean_words)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in clean_words if word not in stop_words]\n",
    "print(f\"\\nüö´ After Removing Stopwords ({len(filtered_words)} tokens):\")\n",
    "print(filtered_words)\n",
    "\n",
    "# Show what stopwords were removed\n",
    "removed_stopwords = [word for word in clean_words if word in stop_words]\n",
    "print(f\"\\nüóëÔ∏è Removed Stopwords ({len(removed_stopwords)} tokens):\")\n",
    "print(removed_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Hands-On Practice: NLTK Stemming & Lemmatization\n",
    "\n",
    "Stemming is a rule-based process that truncates words by removing common prefixes or suffixes. It‚Äôs quick and computationally simple, making it popular for tasks like document classification and search engine indexing.\n",
    "\n",
    "![stem](img/stem.png) \n",
    "\n",
    "Lemmatization takes a more sophisticated route, mapping words to their base or dictionary form (a lemma). Unlike stemming, lemmatization typically requires knowledge of a word‚Äôs part of speech and may rely on morphological analyzers or lexical databases.\n",
    "\n",
    "![lemma](img/lemma.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Initialize stemmers and lemmatizer\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Sample words for demonstration\n",
    "words = ['running', 'runs', 'ran', 'easily', 'fairly', 'studies', 'studying', \n",
    "         'cries', 'crying', 'better', 'best', 'feet', 'geese']\n",
    "\n",
    "print(\"üî§ Stemming vs Lemmatization Comparison:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Original':<12} {'Porter':<12} {'Snowball':<12} {'Lemmatizer':<12}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for word in words:\n",
    "    porter_stem = porter.stem(word)\n",
    "    snowball_stem = snowball.stem(word)\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    \n",
    "    print(f\"{word:<12} {porter_stem:<12} {snowball_stem:<12} {lemma:<12}\")\n",
    "\n",
    "# Let's also try lemmatization with POS tags for better results\n",
    "print(\"\\nüéØ Lemmatization with POS Tags (More Accurate):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "pos_words = [('running', 'v'), ('better', 'a'), ('studies', 'v'), ('studies', 'n')]\n",
    "\n",
    "for word, pos in pos_words:\n",
    "    # Convert POS tag to WordNet format\n",
    "    wordnet_pos = {'n': wordnet.NOUN, 'v': wordnet.VERB, 'a': wordnet.ADJ}.get(pos, wordnet.NOUN)\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "    print(f\"{word} ({pos}) ‚Üí {lemma}\")\n",
    "\n",
    "# Practical example with a sentence\n",
    "sentence = \"The runners were running and studying better strategies\"\n",
    "tokens = word_tokenize(sentence.lower())\n",
    "\n",
    "print(f\"\\nüìù Practical Example:\")\n",
    "print(f\"Original: {sentence}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Stemmed (Porter): {[porter.stem(token) for token in tokens]}\")\n",
    "print(f\"Lemmatized: {[lemmatizer.lemmatize(token) for token in tokens]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4Ô∏è‚É£  spaCy Library Deep Dive\n",
    "\n",
    "## üìñ Theory Overview\n",
    "\n",
    "**spaCy** is an industrial-strength NLP library designed for production use. It's faster than NLTK and provides pre-trained models.\n",
    "\n",
    "### üöÄ spaCy Key Features:\n",
    "- **Fast & Efficient**: Written in Cython\n",
    "- **Pre-trained Models**: Ready-to-use language models\n",
    "- **Advanced NER**: Named Entity Recognition\n",
    "- **Dependency Parsing**: Understanding sentence structure\n",
    "- **Built-in Visualizations**: displaCy for visualization\n",
    "\n",
    "### üìä NLTK vs spaCy Comparison:\n",
    "\n",
    "| Feature | NLTK | spaCy |\n",
    "|---------|------|-------|\n",
    "| Speed | Slower | Faster |\n",
    "| Learning Curve | Steeper | Easier |\n",
    "| Models | Manual setup | Pre-trained |\n",
    "| Production Ready | Research-focused | Production-ready |\n",
    "\n",
    "### üìö Further Reading:\n",
    "- [spaCy Official Documentation](https://spacy.io/)\n",
    "- [spaCy 101 Guide](https://spacy.io/usage/spacy-101)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Hands-On Practice: spaCy Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Apple Inc. is planning to open a new store in New York City next year. \n",
    "Tim Cook, the CEO, announced this during a conference in San Francisco. \n",
    "The company's stock price increased by 5% after the announcement.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"üìù Original Text:\")\n",
    "print(text.strip())\n",
    "\n",
    "# Basic token information\n",
    "print(\"\\nüî§ Token Analysis:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Token':<15} {'Lemma':<15} {'POS':<10} {'Tag':<10} {'Stop?':<8} {'Alpha?'}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for token in doc:\n",
    "    if not token.is_space:  # Skip whitespace tokens\n",
    "        print(f\"{token.text:<15} {token.lemma_:<15} {token.pos_:<10} {token.tag_:<10} {token.is_stop!s:<8} {token.is_alpha}\")\n",
    "\n",
    "# Sentence segmentation\n",
    "print(f\"\\nüìÑ Sentences ({len(list(doc.sents))}):\") \n",
    "for i, sent in enumerate(doc.sents, 1):\n",
    "    print(f\"{i}. {sent.text.strip()}\")\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "print(f\"\\nüè∑Ô∏è Named Entities ({len(doc.ents)}):\")\n",
    "print(\"=\"*50)\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<20} ‚Üí {ent.label_:<10} ({spacy.explain(ent.label_)})\")\n",
    "\n",
    "# Noun phrases\n",
    "print(f\"\\nüìù Noun Phrases ({len(list(doc.noun_chunks)}):\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(f\"- {chunk.text} (root: {chunk.root.text})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Part-of-Speech (POS) Tagging\n",
    "\n",
    "## üìñ Theory Overview\n",
    "\n",
    "**POS Tagging** identifies the grammatical category of each word (noun, verb, adjective, etc.). This is crucial for understanding sentence structure and meaning.\n",
    "\n",
    "### üè∑Ô∏è Common POS Tags:\n",
    "\n",
    "```\n",
    "Sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "The     ‚Üí DT  (Determiner)\n",
    "quick   ‚Üí JJ  (Adjective)\n",
    "brown   ‚Üí JJ  (Adjective) \n",
    "fox     ‚Üí NN  (Noun)\n",
    "jumps   ‚Üí VBZ (Verb, 3rd person singular)\n",
    "over    ‚Üí IN  (Preposition)\n",
    "the     ‚Üí DT  (Determiner)\n",
    "lazy    ‚Üí JJ  (Adjective)\n",
    "dog     ‚Üí NN  (Noun)\n",
    ".       ‚Üí .   (Punctuation)\n",
    "```\n",
    "\n",
    "![POS-Tag](img/POS-Tagging.webp)\n",
    "\n",
    "### üéØ Why POS Tagging Matters:\n",
    "- **Disambiguation**: \"bank\" (financial) vs \"bank\" (river side)\n",
    "- **Information Extraction**: Finding all nouns (entities)\n",
    "- **Grammar Checking**: Ensuring proper sentence structure\n",
    "- **Machine Translation**: Understanding syntax\n",
    "\n",
    "### üìö Further Reading:\n",
    "- [POS-Tagging](https://www.geeksforgeeks.org/nlp/nlp-part-of-speech-default-tagging/)\n",
    "- [Penn Treebank POS Tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Hands-On Practice: POS Tagging Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Sample sentences with different complexities\n",
    "sentences = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"I can can a can into a can.\",  # Ambiguous 'can'\n",
    "    \"The bank near the river bank offers great banking services.\",  # Multiple 'bank'\n",
    "    \"Flying planes can be dangerous.\",  # Ambiguous structure\n",
    "]\n",
    "\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Sentence {i}: {sentence}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # NLTK POS Tagging\n",
    "    nltk_tokens = word_tokenize(sentence)\n",
    "    nltk_pos = nltk.pos_tag(nltk_tokens)\n",
    "    \n",
    "    print(\"\\nüî§ NLTK POS Tagging:\")\n",
    "    for word, pos in nltk_pos:\n",
    "        print(f\"  {word:<12} ‚Üí {pos:<6} ({nltk.help.upenn_tagset(pos) if pos else 'N/A'})\")\n",
    "    \n",
    "    # spaCy POS Tagging\n",
    "    spacy_doc = nlp(sentence)\n",
    "    \n",
    "    print(\"\\nüöÄ spaCy POS Tagging:\")\n",
    "    for token in spacy_doc:\n",
    "        if not token.is_space:\n",
    "            print(f\"  {token.text:<12} ‚Üí {token.pos_:<6} ({spacy.explain(token.pos_) or 'N/A'})\")\n",
    "\n",
    "# POS Distribution Analysis\n",
    "print(\"\\n\\nüìä POS Tag Distribution Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Natural language processing is a subfield of linguistics, computer science, \n",
    "and artificial intelligence concerned with the interactions between computers \n",
    "and human language, in particular how to program computers to process and \n",
    "analyze large amounts of natural language data.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "pos_counts = Counter([token.pos_ for token in doc if not token.is_space])\n",
    "\n",
    "print(f\"Text: {sample_text.strip()[:100]}...\")\n",
    "print(f\"\\nPOS Distribution:\")\n",
    "for pos, count in pos_counts.most_common():\n",
    "    percentage = (count / sum(pos_counts.values())) * 100\n",
    "    print(f\"  {pos:<10} {count:>3} tokens ({percentage:>5.1f}%) - {spacy.explain(pos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Building a Complete Text Preprocessing Pipeline\n",
    "\n",
    "## üìñ Theory Overview\n",
    "\n",
    "A **preprocessing pipeline** chains multiple text cleaning steps together. This ensures consistent, reproducible text processing across your NLP projects.\n",
    "\n",
    "### üîÑ Pipeline Design Pattern:\n",
    "\n",
    "```python\n",
    "def preprocessing_pipeline(text):\n",
    "    text = clean_text(text)\n",
    "    text = tokenize(text) \n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize(text)\n",
    "    return text\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Pipeline Benefits:\n",
    "- **Consistency**: Same processing for all texts\n",
    "- **Modularity**: Easy to add/remove steps\n",
    "- **Debugging**: Can inspect each step\n",
    "- **Scalability**: Can process large datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, use_spacy=True, remove_stopwords=True, lemmatize=True):\n",
    "        \"\"\"Initialize the preprocessor with configuration options\"\"\"\n",
    "        self.use_spacy = use_spacy\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        \n",
    "        # Initialize tools\n",
    "        if use_spacy:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        else:\n",
    "            self.stemmer = PorterStemmer()\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Basic text cleaning\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs, emails, phone numbers\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        text = re.sub(r'\\+?\\d[\\d\\s\\-\\(\\)]+\\d', '', text)\n",
    "        \n",
    "        # Remove social media mentions and hashtags\n",
    "        text = re.sub(r'[@#]\\w+', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def process_with_spacy(self, text):\n",
    "        \"\"\"Process text using spaCy\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        processed_tokens = []\n",
    "        for token in doc:\n",
    "            # Skip spaces, punctuation, and stopwords if configured\n",
    "            if token.is_space or token.is_punct:\n",
    "                continue\n",
    "            if self.remove_stopwords and token.is_stop:\n",
    "                continue\n",
    "            \n",
    "            # Use lemma if configured, otherwise use original token\n",
    "            if self.lemmatize:\n",
    "                processed_tokens.append(token.lemma_)\n",
    "            else:\n",
    "                processed_tokens.append(token.text)\n",
    "        \n",
    "        return processed_tokens\n",
    "    \n",
    "    def process_with_nltk(self, text):\n",
    "        \"\"\"Process text using NLTK\"\"\"\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove punctuation and convert to lowercase\n",
    "        tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "        \n",
    "        # Remove stopwords if configured\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        \n",
    "        # Lemmatize or stem if configured\n",
    "        if self.lemmatize:\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Main preprocessing function\"\"\"\n",
    "        # Step 1: Clean text\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # Step 2: Process tokens\n",
    "        if self.use_spacy:\n",
    "            tokens = self.process_with_spacy(cleaned_text)\n",
    "        else:\n",
    "            tokens = self.process_with_nltk(cleaned_text)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def preprocess_documents(self, documents, show_progress=True):\n",
    "        \"\"\"Process multiple documents\"\"\"\n",
    "        processed_docs = []\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            if show_progress and i % 10 == 0:\n",
    "                print(f\"Processing document {i+1}/{len(documents)}\")\n",
    "            \n",
    "            processed_tokens = self.preprocess(doc)\n",
    "            processed_docs.append(processed_tokens)\n",
    "        \n",
    "        return processed_docs\n",
    "\n",
    "# Demo the preprocessing pipeline\n",
    "print(\"üîß Text Preprocessing Pipeline Demo\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sample documents\n",
    "sample_docs = [\n",
    "    \"Hello World! I'm learning NLP and it's AMAZING!!! Visit https://example.com\",\n",
    "    \"Natural Language Processing helps computers understand human language.\",\n",
    "    \"Machine Learning and AI are transforming technology @TechExpert #AI\",\n",
    "    \"The quick brown fox jumps over the lazy dog. Email: test@email.com\"\n",
    "]\n",
    "\n",
    "# Initialize different preprocessors\n",
    "preprocessors = {\n",
    "    \"spaCy (Full)\": TextPreprocessor(use_spacy=True, remove_stopwords=True, lemmatize=True),\n",
    "    \"spaCy (No Stopwords)\": TextPreprocessor(use_spacy=True, remove_stopwords=False, lemmatize=True),\n",
    "    \"NLTK (Full)\": TextPreprocessor(use_spacy=False, remove_stopwords=True, lemmatize=True),\n",
    "}\n",
    "\n",
    "# Compare different preprocessing approaches\n",
    "for doc_idx, doc in enumerate(sample_docs):\n",
    "    print(f\"\\nüìÑ Document {doc_idx + 1}:\")\n",
    "    print(f\"Original: {doc}\")\n",
    "    \n",
    "    for name, preprocessor in preprocessors.items():\n",
    "        tokens = preprocessor.preprocess(doc)\n",
    "        print(f\"{name:>15}: {tokens}\")\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\n‚è±Ô∏è Performance Comparison:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "import time\n",
    "\n",
    "large_text = \" \".join(sample_docs) * 100  # Create larger text for timing\n",
    "\n",
    "for name, preprocessor in preprocessors.items():\n",
    "    start_time = time.time()\n",
    "    result = preprocessor.preprocess(large_text)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    processing_time = (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "    print(f\"{name:>15}: {processing_time:.2f}ms ({len(result)} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ Advanced NLP Techniques\n",
    "\n",
    "Beyond basic preprocessing, there are several advanced techniques which help computers understand words by converting them to numbers (numerical features), this hugely improves model performance:\n",
    "\n",
    "### 1. Bag of Words (BoW)\n",
    "\n",
    "A simple and popular technique used to convert text into numerical features for machine learning models to recognize words by counting how many times each word appears in the document.\n",
    "\n",
    "![bow](img/bow.png)\n",
    "\n",
    "### 2. TF-IDF (Term Frequency‚ÄìInverse Document Frequency)\n",
    "This is a technique that scores each word in a document based on how often it appears in that document (TF) and how rare it is across all documents (IDF). It improves Bag of Words by reducing the weight of common words (like ‚Äúthe‚Äù, ‚Äúis‚Äù) and highlighting more informative, distinctive terms.\n",
    "\n",
    "![tf-idf](img/tf-idf.png)\n",
    "![tf-idf2](img/tf-idf2.png)\n",
    "\n",
    "### 3. N-grams\n",
    "An n-gram is a contiguous sequence of n words from a text, such as bigrams (2-grams) or trigrams (3-grams), capturing word combinations. It helps by preserving some context and word order, improving models‚Äô ability to understand phrases and local dependencies.\n",
    "\n",
    "![ngram](img/ngram.png)\n",
    "\n",
    "\n",
    "\n",
    "1. **Word Embeddings**: Dense vector representations\n",
    "2. **Dependency Parsing**: Understanding sentence structure\n",
    "\n",
    "### üìö Further Reading:\n",
    "- [Bag of Words](https://www.geeksforgeeks.org/nlp/bag-of-words-bow-model-in-nlp/)\n",
    "- [N-grams Explained](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n",
    "- [TF-IDF Tutorial](https://www.geeksforgeeks.org/machine-learning/understanding-tf-idf-term-frequency-inverse-document-frequency/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Hands-On Practice: BoW, N-grams and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW Implementation \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences = [\"I love cats\", \"I hate dogs\"]\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')  # Adjusted pattern to include single characters\n",
    "bow_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"Vectors:\\n\", bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus for analysis\n",
    "corpus = [\n",
    "    \"Natural language processing is amazing and useful for text analysis\",\n",
    "    \"Machine learning algorithms can process natural language effectively\", \n",
    "    \"Text analysis and natural language understanding are key AI technologies\",\n",
    "    \"Deep learning models excel at natural language processing tasks\",\n",
    "    \"Natural language generation is an exciting field in artificial intelligence\"\n",
    "]\n",
    "\n",
    "print(\"üìö Sample Corpus:\")\n",
    "for i, doc in enumerate(corpus, 1):\n",
    "    print(f\"{i}. {doc}\")\n",
    "\n",
    "# N-grams Analysis\n",
    "print(\"\\n\\nüîó N-grams Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Combine all documents for n-gram analysis\n",
    "all_text = \" \".join(corpus).lower()\n",
    "tokens = word_tokenize(all_text)\n",
    "tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "# Generate different n-grams\n",
    "for n in [1, 2, 3]:\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    n_gram_freq = Counter(n_grams)\n",
    "    \n",
    "    print(f\"\\n{n}-grams (Top 5):\")\n",
    "    for gram, freq in n_gram_freq.most_common(5):\n",
    "        gram_str = \" \".join(gram)\n",
    "        print(f\"  '{gram_str}': {freq}\")\n",
    "\n",
    "# TF-IDF Analysis\n",
    "print(\"\\n\\nüìä TF-IDF Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=20,  # Limit to top 20 features\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2)  # Include both unigrams and bigrams\n",
    ")\n",
    "\n",
    "# Fit and transform corpus\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=feature_names,\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(corpus))]\n",
    ")\n",
    "\n",
    "print(\"TF-IDF Scores (Top features per document):\")\n",
    "for idx, row in tfidf_df.iterrows():\n",
    "    top_features = row.nlargest(3)\n",
    "    print(f\"\\n{idx}:\")\n",
    "    for feature, score in top_features.items():\n",
    "        if score > 0:\n",
    "            print(f\"  {feature}: {score:.3f}\")\n",
    "\n",
    "# Visualize TF-IDF matrix\n",
    "print(\"\\nüìà TF-IDF Matrix Visualization:\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(tfidf_df.T, annot=True, fmt='.2f', cmap='YlOrRd', cbar=True)\n",
    "plt.title('TF-IDF Scores Heatmap')\n",
    "plt.xlabel('Documents')\n",
    "plt.ylabel('Terms')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare with simple word counts\n",
    "count_vectorizer = CountVectorizer(stop_words='english', max_features=10)\n",
    "count_matrix = count_vectorizer.fit_transform(corpus)\n",
    "count_df = pd.DataFrame(\n",
    "    count_matrix.toarray(),\n",
    "    columns=count_vectorizer.get_feature_names_out(),\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(corpus))]\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Word Count vs TF-IDF Comparison:\")\n",
    "print(\"Word Counts:\")\n",
    "print(count_df.sum().sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nAverage TF-IDF Scores:\")\n",
    "common_features = set(count_df.columns) & set(tfidf_df.columns)\n",
    "for feature in sorted(common_features):\n",
    "    avg_tfidf = tfidf_df[feature].mean()\n",
    "    total_count = count_df[feature].sum()\n",
    "    print(f\"{feature}: Count={total_count}, Avg TF-IDF={avg_tfidf:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ Mini Project: Sentiment Analysis Model\n",
    "\n",
    "## üìñ Project Overview\n",
    "\n",
    "Now we'll combine everything we've learned to build a complete **Sentiment Analysis** model! This project will:\n",
    "\n",
    "1. **Load and explore** a sentiment dataset\n",
    "2. **Preprocess** the text data using our pipeline\n",
    "3. **Extract features** using TF-IDF\n",
    "4. **Train multiple models** (Naive Bayes, SVM, Logistic Regression)\n",
    "5. **Evaluate performance** and compare models\n",
    "6. **Create a prediction interface** for new text\n",
    "\n",
    "### üéØ Learning Goals:\n",
    "- Apply preprocessing techniques to real data\n",
    "- Understand feature extraction for ML models\n",
    "- Compare different classification algorithms\n",
    "- Build an end-to-end NLP application\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 1: Dataset Creation and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample sentiment dataset\n",
    "# In practice, you might use datasets like IMDB, Amazon reviews, or Twitter sentiment\n",
    "\n",
    "sample_data = {\n",
    "    'text': [\n",
    "        # Positive examples\n",
    "        \"I absolutely love this product! It's amazing and works perfectly.\",\n",
    "        \"Great experience! Highly recommend to everyone.\",\n",
    "        \"Fantastic quality and excellent customer service.\",\n",
    "        \"This is the best purchase I've made this year!\",\n",
    "        \"Outstanding performance and great value for money.\",\n",
    "        \"I'm so happy with this product. It exceeded my expectations.\",\n",
    "        \"Wonderful design and very user-friendly interface.\",\n",
    "        \"Perfect! Exactly what I was looking for.\",\n",
    "        \"Excellent build quality and fast delivery.\",\n",
    "        \"Amazing features and works flawlessly.\",\n",
    "        \n",
    "        # Negative examples\n",
    "        \"This product is terrible and doesn't work at all.\",\n",
    "        \"Worst purchase ever! Complete waste of money.\",\n",
    "        \"Poor quality and breaks easily. Very disappointed.\",\n",
    "        \"Horrible customer service and defective product.\",\n",
    "        \"I hate this product. It's completely useless.\",\n",
    "        \"Terrible experience. Would not recommend to anyone.\",\n",
    "        \"Very poor quality and overpriced. Avoid at all costs.\",\n",
    "        \"Disappointing performance and many issues.\",\n",
    "        \"Awful design and difficult to use.\",\n",
    "        \"Complete failure. Doesn't meet basic requirements.\",\n",
    "        \n",
    "        # Neutral examples\n",
    "        \"The product is okay, nothing special but works fine.\",\n",
    "        \"Average quality, meets basic requirements but not impressive.\",\n",
    "        \"It's an okay product for the price. Nothing more, nothing less.\",\n",
    "        \"Decent build quality but could be better.\",\n",
    "        \"Reasonable performance, though there are some minor issues.\",\n",
    "        \"The product works as expected, no major complaints.\",\n",
    "        \"Fair quality and standard features.\",\n",
    "        \"Acceptable product with room for improvement.\",\n",
    "        \"It does the job but isn't particularly exciting.\",\n",
    "        \"Standard quality product with basic functionality.\"\n",
    "    ],\n",
    "    'sentiment': [\n",
    "        # Positive labels (1)\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        # Negative labels (0)\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        # Neutral labels (2)\n",
    "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(sample_data)\n",
    "\n",
    "# Map numeric labels to text for better understanding\n",
    "sentiment_map = {0: 'Negative', 1: 'Positive', 2: 'Neutral'}\n",
    "df['sentiment_text'] = df['sentiment'].map(sentiment_map)\n",
    "\n",
    "print(\"üìä Dataset Overview:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Features: {list(df.columns)}\")\n",
    "\n",
    "print(\"\\nüìà Sentiment Distribution:\")\n",
    "sentiment_counts = df['sentiment_text'].value_counts()\n",
    "print(sentiment_counts)\n",
    "\n",
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sentiment_counts.plot(kind='bar', color=['red', 'green', 'gray'])\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%', \n",
    "        colors=['red', 'green', 'gray'])\n",
    "plt.title('Sentiment Distribution (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nüìÑ Sample Data:\")\n",
    "for i, row in df.head(3).iterrows():\n",
    "    print(f\"\\n{i+1}. Sentiment: {row['sentiment_text']}\")\n",
    "    print(f\"   Text: {row['text']}\")\n",
    "\n",
    "# Text length analysis\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "print(\"\\nüìè Text Statistics:\")\n",
    "print(f\"Average text length: {df['text_length'].mean():.1f} characters\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.1f} words\")\n",
    "\n",
    "# Text length by sentiment\n",
    "print(\"\\nüìä Text Length by Sentiment:\")\n",
    "length_by_sentiment = df.groupby('sentiment_text')[['text_length', 'word_count']].mean()\n",
    "print(length_by_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 2: Text Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply our preprocessing pipeline to the dataset\n",
    "print(\"üîß Applying Text Preprocessing Pipeline...\")\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(use_spacy=True, remove_stopwords=True, lemmatize=True)\n",
    "\n",
    "# Preprocess all texts\n",
    "processed_texts = []\n",
    "for text in df['text']:\n",
    "    tokens = preprocessor.preprocess(text)\n",
    "    processed_text = ' '.join(tokens)  # Join tokens back to string\n",
    "    processed_texts.append(processed_text)\n",
    "\n",
    "df['processed_text'] = processed_texts\n",
    "\n",
    "# Show preprocessing results\n",
    "print(\"\\nüìÑ Preprocessing Examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n{i+1}. Original: {df.iloc[i]['text']}\")\n",
    "    print(f\"   Processed: {df.iloc[i]['processed_text']}\")\n",
    "\n",
    "# Compare text lengths before and after preprocessing\n",
    "df['processed_length'] = df['processed_text'].str.len()\n",
    "df['processed_word_count'] = df['processed_text'].str.split().str.len()\n",
    "\n",
    "print(\"\\nüìä Preprocessing Impact:\")\n",
    "print(f\"Average length reduction: {df['text_length'].mean() - df['processed_length'].mean():.1f} characters\")\n",
    "print(f\"Average word reduction: {df['word_count'].mean() - df['processed_word_count'].mean():.1f} words\")\n",
    "\n",
    "# Visualize the impact\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(df['text_length'], df['processed_length'], alpha=0.6)\n",
    "plt.plot([0, df['text_length'].max()], [0, df['text_length'].max()], 'r--', alpha=0.5)\n",
    "plt.xlabel('Original Text Length')\n",
    "plt.ylabel('Processed Text Length')\n",
    "plt.title('Text Length: Before vs After Preprocessing')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(df['word_count'], df['processed_word_count'], alpha=0.6)\n",
    "plt.plot([0, df['word_count'].max()], [0, df['word_count'].max()], 'r--', alpha=0.5)\n",
    "plt.xlabel('Original Word Count')\n",
    "plt.ylabel('Processed Word Count')\n",
    "plt.title('Word Count: Before vs After Preprocessing')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create word cloud for each sentiment\n",
    "print(\"\\n‚òÅÔ∏è Word Clouds by Sentiment:\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "colors = ['red', 'green', 'gray']\n",
    "\n",
    "for i, (sentiment, color) in enumerate(zip(['Negative', 'Positive', 'Neutral'], colors)):\n",
    "    # Combine all processed texts for this sentiment\n",
    "    sentiment_texts = ' '.join(df[df['sentiment_text'] == sentiment]['processed_text'])\n",
    "    \n",
    "    if sentiment_texts.strip():  # Check if there's text to process\n",
    "        wordcloud = WordCloud(\n",
    "            width=400, height=300, \n",
    "            background_color='white',\n",
    "            colormap=plt.cm.get_cmap('Reds' if sentiment == 'Negative' else \n",
    "                                    'Greens' if sentiment == 'Positive' else 'Greys')\n",
    "        ).generate(sentiment_texts)\n",
    "        \n",
    "        axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "        axes[i].set_title(f'{sentiment} Sentiment', fontsize=14, color=color)\n",
    "        axes[i].axis('off')\n",
    "    else:\n",
    "        axes[i].text(0.5, 0.5, 'No text data', ha='center', va='center', \n",
    "                    transform=axes[i].transAxes)\n",
    "        axes[i].set_title(f'{sentiment} Sentiment', fontsize=14, color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 3: Feature Extraction and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"ü§ñ Machine Learning Model Training\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Prepare data\n",
    "X = df['processed_text']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Training sentiment distribution:\\n{pd.Series(y_train).value_counts()}\")\n",
    "\n",
    "# Define models with pipelines\n",
    "models = {\n",
    "    'Naive Bayes': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ]),\n",
    "    \n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "        ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "    ]),\n",
    "    \n",
    "    'SVM': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "        ('classifier', SVC(random_state=42, probability=True))\n",
    "    ]),\n",
    "    \n",
    "    'Random Forest': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"\\nüîÑ Training Models...\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
